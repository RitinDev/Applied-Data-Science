{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy_8z4fU3dv"
      },
      "source": [
        "---\n",
        "---\n",
        "Problem Set 6: Web Scraping\n",
        "\n",
        "Applied Data Science using Python\n",
        "\n",
        "New York University, Abu Dhabi\n",
        "\n",
        "Out: 7th Dec 2023 || **Due: 14th Dec 2023 at 23:59**\n",
        "\n",
        "---\n",
        "---\n",
        "#Start Here\n",
        "## Learning Goals\n",
        "### General Goals\n",
        "- Learn the fundamental concepts of Web Scraping\n",
        "\n",
        "### Specific Goals\n",
        "- Learn how to identify websites that allow scraping\n",
        "- Learn the basics of BeautifulSoup\n",
        "- Learn to adjust the scraping code to more than one query\n",
        "\n",
        "## Collaboration Policy\n",
        "- You are expected to comply with the [University Policy on Academic Integrity and Plagiarism](https://www.nyu.edu/about/policies-guidelines-compliance/policies-and-guidelines/academic-integrity-for-students-at-nyu.html).\n",
        "- You are allowed to talk with / work with other students on homework assignments.\n",
        "- You can share ideas but not code, analyses or results; you must submit your own code and results. All submitted code will be compared against all code submitted this semester and online using MOSS. We will also critically analyze the similarities in the submitted reports, methodologies, and results, **but we will not police you**. We expect you all to be mature and responsible enough to finish your work with full integrity.\n",
        "\n",
        "## Late Submission Policy\n",
        "You can submit the homework for upto 3 late days. However, we will deduct **20 points** from your homework grade **for each late day you take**. We will not accept the homework after 3 late days.\n",
        "\n",
        "## Distribution of Class Materials\n",
        "These problem sets and recitations are intellectual property of NYUAD, and we request the students to **not** distribute them or their solutions to other students who have not signed up for this class, and/or intend to sign up in the future. We also request you don't post these problem sets, and recitations online or on any public platforms.\n",
        "\n",
        "## Disclaimer\n",
        "The number of points do not necessarily signify/correlate to the difficulty level of the tasks.\n",
        "\n",
        "## Submission\n",
        "You will submit all your code as a Python Notebook through [Brightspace](https://brightspace.nyu.edu/) as **P6_YOUR NETID.ipynb**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntO7OjkhUOvp"
      },
      "source": [
        "# General Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzxk0RpUyTCC"
      },
      "source": [
        "This homework is worth 100 points. It has 4 parts. Below each part, we provide a set of concepts required to complete that part. All the parts need to be completed in a Jupyter (Colab) Notebook attached with this handout.\n",
        "\n",
        "<font color=\"red\">**Important Note:** Please scrape the websites ethically and make sure you utilize the `sleep` function between requests. </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWE1SZaqQm31"
      },
      "source": [
        "# Part I: To Scrape or Not to Scrape (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p11YK7l3TXWm"
      },
      "source": [
        "A friend wants us to write a function that will scrape product data from 3 different websites, and will recommend which one to buy based on price and reviews. However, we need to figure out which of the below websites actually allow scraping:\n",
        "\n",
        "- [Amazon.ae](https://www.amazon.ae/)\n",
        "- [Noon.com](https://www.noon.com/uae-en/)\n",
        "- [Instock.ae](https://www.instock.ae/)\n",
        "\n",
        "Provide your answers, and, in a couple of sentences, describe how you have come to your answer.\n",
        "\n",
        "*Hint: Two websites allow us to scrape their search results.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the analysis of the `robots.txt` files from the three websites, here are the conclusions regarding their scraping policies:\n",
        "\n",
        "1. **[Amazon.ae](https://www.amazon.ae/)**: Amazon.ae's `robots.txt` file is quite restrictive. It disallows scraping in many parts of the website, including product, account, cart, and user interaction pages. Specific bots are entirely blocked. Thus, Amazon.ae does not generally allow scraping for product data.\n",
        "\n",
        "2. **[Noon.com](https://www.noon.com/uae-en/)**: Noon.com's `robots.txt` file is more permissive. It only disallows access to paths under `/_svc/` while allowing all other paths. This indicates that Noon.com generally allows scraping, including for product data.\n",
        "\n",
        "3. **[Instock.ae](https://www.instock.ae/)**: Instock.ae's `robots.txt` file restricts access to various parts of the site, including specific directories and functionalities related to products and customer interactions. However, it does not explicitly disallow scraping of product data from search results or general product pages.\n",
        "\n",
        "Therefore, based on the `robots.txt` analysis, Noon.com and Instock.ae allow scraping of their search results to some extent, while Amazon.ae does not appear to permit scraping for product data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7dlpv62CDum"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Concepts of Ethical Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DojUO82FXqEv"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +5 points for correct answer and pointing the place you found the information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrIBbFv7UHyj"
      },
      "source": [
        "# Part II: Hindi Geet Mala (40 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBCZYVp67t13"
      },
      "source": [
        "[Hindi Geet Mala](https://www.hindigeetmala.net/) is a website containing information about Indian movies, songs, singers, etc. For this part, you will scrape information about all the movies in alphabetical order. Additionally, you will scrape information about songs.\n",
        "\n",
        "More precisely, you will submit 2 CSV files:\n",
        "\n",
        "* movies.csv: Title, Year, Number of Songs, Film Director, Film Producer, Film cast, Lyricist, Music Director, Singer, External links, Watch Full Movie\n",
        "\n",
        "* songs.csv: Artists, Title, Rating, Number of Votes, Movie Title\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- you are NOT allowed to hardcode the list of letters.\n",
        "- you are NOT allowed to use Pandas function `read_html()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYKOgyUxr0HU"
      },
      "outputs": [],
      "source": [
        "# Write you solution here\n",
        "############# SOLUTION ###############\n",
        "\n",
        "############# SOLUTION END ###############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON16kQeAAYgm"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Navigating through HTML code using functions\n",
        "- DataFrame Creation and Writing to a file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-gC50LgGO2"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +30 points for correct output (15 points for each dataframe)\n",
        "- +5 points for concise, logical code\n",
        "- +3 points for ethical and mindful scraping\n",
        "- +2 points for comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC4EGarx7vh9"
      },
      "source": [
        "# Part III: Yahoo Finance (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZELErl5z7yqA"
      },
      "source": [
        "A friend who studies Economics, recommended us to buy stocks if the weekly average of closing price has been steadily increasing by at least 1% for the last 3 weeks.\n",
        "\n",
        "Since we are interested in more than one company, let's write a function called `stock_decision()` that will scrape stock market close data for the given companies from [Yahoo Finance](https://finance.yahoo.com/), compute the weekly average for the past 3 weeks and check whether or not it has been increasing. If the weekly average has been steadily increasing by at least 1%, recommend to buy, otherwise recommend to sell.\n",
        "\n",
        "The output should be two lines:\n",
        "\n",
        "```\n",
        "\"Sell: \"\n",
        "\n",
        "\"Buy: \"\n",
        "```\n",
        "\n",
        "Implement the function for these companies.\n",
        "- Apple\n",
        "- Microsoft\n",
        "- Amazon\n",
        "- Tesla\n",
        "- Facebook\n",
        "\n",
        "For example, the url for Facebook would be https://finance.yahoo.com/quote/FB/history/.\n",
        "\n",
        "\n",
        "\n",
        "**Hint:** For grouping the data per week, look up `resample()` function from Pandas. Another function that will come in handy is `pct_change()` from Pandas.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- you are NOT allowed to use Pandas function `read_html()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ShRFrKvGsPfX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sell: AAPL, MSFT, AMZN, TSLA, FB\n",
            "Buy: \n"
          ]
        }
      ],
      "source": [
        "# Write you solution here\n",
        "############# SOLUTION ###############\n",
        "def fetch_stock_data(symbol):\n",
        "    # Create a stock prices dataframe with columns: Date, Open, High, Low, Close, Adj Close, Volume\n",
        "    stock_data = pd.DataFrame(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
        "    \n",
        "    # Construct URL for historical data of the stock symbol\n",
        "    url = f\"https://finance.yahoo.com/quote/{symbol}/history/\"\n",
        "    response = requests.get(url, headers=header)\n",
        "    soup = bs(response.text, 'html.parser')    \n",
        "    \n",
        "    # Historical data is in a table with data-test attribute set to 'historical-prices'\n",
        "    table = soup.find('table', attrs={'data-test': 'historical-prices'})\n",
        "    tbody = table.find('tbody')\n",
        "    rows = tbody.find_all('tr')\n",
        "    \n",
        "    # Iterate through each row and append the data to the stock_data dataframe\n",
        "    for row in rows:\n",
        "        # Each row has 7 columns\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) == 7:\n",
        "            # Extract the text from each column and append to the stock_data dataframe\n",
        "            stock_data = pd.concat([stock_data, pd.DataFrame([[cols[0].text, cols[1].text, cols[2].text, cols[3].text, cols[4].text, cols[5].text, cols[6].text]], columns=['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])], ignore_index=True)\n",
        "\n",
        "    return stock_data\n",
        "\n",
        "def process_data(df):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df.set_index('Date', inplace=True)\n",
        "    \n",
        "    # Convert 'Close' to numeric, coerce errors\n",
        "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
        "\n",
        "    # Drop rows with NaN values (if any)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Resample to get weekly averages and calculate percentage change\n",
        "    weekly_avg = df['Close'].resample('W').mean()\n",
        "    pct_change = weekly_avg.pct_change()\n",
        "\n",
        "    return pct_change\n",
        "\n",
        "def stock_decision(companies):\n",
        "    decisions = {'Buy': [], 'Sell': []}\n",
        "\n",
        "    for company in companies:\n",
        "        stock_data = fetch_stock_data(company)\n",
        "        pct_change = process_data(stock_data)\n",
        "\n",
        "        # Check if the weekly average has been steadily increasing by at least 1% for the last 3 weeks\n",
        "        if all(pct_change[-3:] >= 0.01):\n",
        "            decisions['Buy'].append(company)\n",
        "        else:\n",
        "            decisions['Sell'].append(company)\n",
        "\n",
        "    return decisions\n",
        "\n",
        "# Test the function with the specified companies\n",
        "companies = ['AAPL', 'MSFT', 'AMZN', 'TSLA', 'FB']\n",
        "# companies = ['SONO'] # Uncomment this line to test with a single company that is doing well enough to buy\n",
        "decisions = stock_decision(companies)\n",
        "print(\"Sell:\", \", \".join(decisions['Sell']))\n",
        "print(\"Buy:\", \", \".join(decisions['Buy']))\n",
        "############# SOLUTION END ###############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgbfC_N7BbFV"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Navigating through HTML code using functions\n",
        "- DataFrame Creation and Manipulations\n",
        "- Applying functions to a DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqnMGCkcOd-O"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +20 points for correct output\n",
        "- +5 points for concise and logical code\n",
        "- +3 points for ethical and mindful scraping\n",
        "- +2 points for comments and logical variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaTNEZB3bNVX"
      },
      "source": [
        "# Part IV: WikiCFP (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYkNHFZtbU8L"
      },
      "source": [
        "[WikiCFP](https://wikicfp.com/) is a semantic wiki for Call for Papers in science and technology fields.\n",
        "\n",
        "For this part, you will be writing a program that will do the following:\n",
        "\n",
        "* scrape the first 10 pages of WikiCFP for the keyword `computer science`\n",
        "* create a Pandas DataFrame with the columns `abbreviation, name, dates, place, deadline`\n",
        "* create a new column called `country`, which will take the value `online` if the place contains `Online` or `Virtual`.\n",
        "* remove missing values, and calculate the distribution of places and print the top and bottom countries.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- you are NOT allowed to use Pandas function `read_html()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "oOE6cYubsRw6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top Countries:\n",
            " Australia      27\n",
            "online         26\n",
            "Switzerland    16\n",
            "China          16\n",
            "UAE            13\n",
            "Name: country, dtype: int64\n",
            "\n",
            "Bottom Countries:\n",
            " USA                       2\n",
            "Indonesia                 2\n",
            "Netherlands and online    1\n",
            "India                     1\n",
            "Algiers Algeria           1\n",
            "Name: country, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Write you solution here\n",
        "############# SOLUTION ###############\n",
        "def scrape_wikicfp(keyword, pages=1):\n",
        "    base_url = 'http://www.wikicfp.com/cfp/call?conference='\n",
        "    df = pd.DataFrame(columns=['abbreviation', 'name', 'dates', 'place', 'deadline'])\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = f\"{base_url}{keyword}&page={page}\"\n",
        "        time.sleep(1) # Some etiquette to not overload the server\n",
        "        response = requests.get(url, headers=header)\n",
        "        soup = bs(response.text, 'html.parser')\n",
        "        \n",
        "        events_table = soup.find('table', attrs={'cellpadding': '3', 'cellspacing': '1', 'width': '100%'})\n",
        "        \n",
        "        # Now, find TRs with bgcolor attribute set to '#f6f6f6' and '#e6e6e6'\n",
        "        event_details_trs = events_table.find_all('tr', attrs={'bgcolor': ['#f6f6f6', '#e6e6e6']})\n",
        "        \n",
        "        # Now, split the event_details_trs into chunks of 2\n",
        "        event_details = [event_details_trs[i:i+2] for i in range(0, len(event_details_trs), 2)]\n",
        "        \n",
        "        # Now, inside each event_details, find the tds with ONLY align attribute set to 'left'\n",
        "        for event in event_details:\n",
        "            first_row = event[0].find_all('td', attrs={'align': 'left'})\n",
        "            second_row = event[1].find_all('td', attrs={'align': 'left'})\n",
        "            \n",
        "            event_abbreviation = first_row[0].find('a').text\n",
        "            event_name = first_row[1].text\n",
        "            \n",
        "            event_dates = second_row[0].text\n",
        "            event_place = second_row[1].text\n",
        "            event_deadline = second_row[2].text\n",
        "            \n",
        "            df = pd.concat([df, pd.DataFrame([[event_abbreviation, event_name, event_dates, event_place, event_deadline]], columns=['abbreviation', 'name', 'dates', 'place', 'deadline'])], ignore_index=True)\n",
        "\n",
        "    # Create 'country' column\n",
        "    df['country'] = df['place'].apply(lambda x: 'online' if 'Virtual' in x or 'Online' in x else x.split(',')[-1].strip())\n",
        "    \n",
        "    # If country is 'N/A', remove the row\n",
        "    df = df[df['country'] != 'N/A']\n",
        "\n",
        "    return df\n",
        "\n",
        "# # Usage\n",
        "keyword = 'computer%20science'\n",
        "df = scrape_wikicfp(keyword, pages=10)\n",
        "df\n",
        "\n",
        "# Analyze distribution of places\n",
        "place_distribution = df['country'].value_counts()\n",
        "print(\"Top Countries:\\n\", place_distribution.head())\n",
        "print(\"\\nBottom Countries:\\n\", place_distribution.tail())\n",
        "############# SOLUTION END ###############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gFwiejBBpCV"
      },
      "source": [
        "### *Concepts required to complete this task:*\n",
        "\n",
        "- Navigating through HTML code using functions\n",
        "- DataFrame Creation\n",
        "- Adding new columns to an existing DataFrame\n",
        "- String operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs9iuLEYPHuk"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +17 points for correct output\n",
        "- +3 points for concise and logical code\n",
        "- +3 points for ethical and mindful scraping\n",
        "- +2 points for comments and logical variable names"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
