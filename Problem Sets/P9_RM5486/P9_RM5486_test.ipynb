{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxdctIdxU6B"
      },
      "source": [
        "---\n",
        "---\n",
        "Problem Set 10: Machine Learning II\n",
        "\n",
        "Applied Data Science using Python\n",
        "\n",
        "New York University, Abu Dhabi\n",
        "\n",
        "Out: 21st Nov 2023 || **Due: 28th Nov 2023 at 23:59**\n",
        "\n",
        "---\n",
        "---\n",
        "#Start Here\n",
        "## Learning Goals\n",
        "### General Goals\n",
        "- Learn the fundamental concepts of applied machine learning\n",
        "- Learn the fundamental concepts of supervised learning\n",
        "\n",
        "### Specific Goals\n",
        "- Learn the basics of regression\n",
        "- Learn to apply different models of regression:\n",
        "    - linear regression\n",
        "    - polynomial regression\n",
        "    - kNN regression\n",
        "- Understand bias-variance tradeoff\n",
        "- Learn to apply cross validation\n",
        "- Learn to apply regularization (L1 vs. L2)\n",
        "- Learn to evaluate and compare the performance of your regression models\n",
        "- Learn to apply feature scaling\n",
        "- Feature engineering\n",
        "- Understand transfer learning\n",
        "\n",
        "## Collaboration Policy\n",
        "- You are allowed to talk with / work with other students on homework assignments.\n",
        "- You can share ideas but not code, analyses or results; you must submit your own code and results. All submitted code will be compared against all code submitted this and previous semesters and online using MOSS. We will also critically analyze the similarities in the submitted reports, methodologies, and results, **but we will not police you**. We expect you all to be mature and responsible enough to finish your work with full integrity.\n",
        "- You are expected to comply with the [University Policy on Academic Integrity and Plagiarism](https://www.nyu.edu/about/policies-guidelines-compliance/policies-and-guidelines/academic-integrity-for-students-at-nyu.html). Violations may result in penalties, such as failure in a particular assignment.\n",
        "\n",
        "## Distribution of Class Materials\n",
        "These problem sets and recitations are intellectual property of NYUAD, and we request the students to **not** distribute them or their solutions to other students who have not signed up for this class, and/or intend to sign up in the future. We also request you don't post these problem sets, and recitations online or on any public platforms.\n",
        "\n",
        "## Late Submission Policy\n",
        "You can submit the homework for upto 3 late days. However, we will deduct **20 points** from your homework grade **for each late day you take**. We will not accept the homework after 3 late days.\n",
        "\n",
        "\n",
        "## Disclaimer\n",
        "The number of points do not necessarily signify/correlate to the difficulty level of the tasks.\n",
        "\n",
        "## Submission\n",
        "You will submit all your code as a Python Notebook through [Brightspace](https://brightspace.nyu.edu/) as **P10_YOUR NETID.ipynb**.\n",
        "\n",
        "## Kaggle Username\n",
        "\n",
        "[ENTER YOUR KAGGLE USERNAME HERE]\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMDlF4mjG6v"
      },
      "source": [
        "# General Instructions\n",
        "This homework is worth 100 points. It has 2 parts. Below each part, we provide a set of concepts required to complete that part. All the parts need to be completed in this Jupyter (Colab) Notebook. Please start this homework early as modeling may take some time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWf22cCjQNY"
      },
      "source": [
        "# Part I: Predicting the Prevalence of the CCD Disease (70 points)\n",
        "\n",
        "For a long time now, humans of the **United States of America (USA)** have been suffering from a communicable disease called the CCD, short for the **Climate Change Denialism**, a serious disease that is making humans incapable to reason. True story! <sup>1</sup>\n",
        "\n",
        "The Center of Logical Reasoning has been collecting the data related to the disease since 2010, and has reached out to NYU for help in creating a model for the prediction of the prevalance of **Climate Change Denialism** in different states using a set of features. The dataset is **spatio-temporal** as it has prevalance rates of the disease for ~50 states (spatial), across 7 years (temporal).\n",
        "\n",
        "------------------\n",
        "<sup>1. This is a work of fiction. The story, names, writing, data depicted in this problem set are mostly ficticious. Any similarity to actual persons, living or dead, or to actual papers, is not purely coincidental but definitely inspirational. The \"Climate Change Denialism\" is a fictitious disease that may have been inspired by a same name disorder found amongst certain individuals in the world.</sup>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfGNVsfFlYHV"
      },
      "source": [
        "## A. Training for the US (35 points)\n",
        "\n",
        "Using the dataset `us_train.csv`, train a machine learning based regression model that predicts the prevalence of **Climate Change Denialism** disease for a particular state in the USA. The features are in the columns labeled as `A`, `B`, ..., `AC`. The outcome variable (i.e. the prevalence of CCD disease) is present in the column `outcome`.\n",
        "\n",
        "You may try different models (linear, polynomial, kNN) to see which one performs the best for estimation of the prevalence of the disease. You have data for the years 2010 to 2015 for 50 states in the U.S. Your data will be tested on data from 2016. The features for 2016 are provided in the file `us_test_x.csv`. The outcome/labels for 2016 are not provided.\n",
        "\n",
        "For this part, you are required to train and evaluate your regression models very similar to what we did in the recitation.\n",
        "\n",
        "As a submission for this part, you will fill the `us_predictions.csv` file and submit that along with this Notebook to Brightspace. You will also submit `us_predictions.csv` file to Kaggle (see Part B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "0DHhFABDkXB6"
      },
      "outputs": [],
      "source": [
        "# Importing libraries you \"may\" need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1: Load dataset and sample it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {
        "id": "5lHr1i-Eq2ID"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>states</th>\n",
              "      <th>year</th>\n",
              "      <th>outcome</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>...</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>AA</th>\n",
              "      <th>AB</th>\n",
              "      <th>AC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Wyoming,2010</td>\n",
              "      <td>Wyoming</td>\n",
              "      <td>2010</td>\n",
              "      <td>7.2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>Iowa,2012</td>\n",
              "      <td>Iowa</td>\n",
              "      <td>2012</td>\n",
              "      <td>9.7</td>\n",
              "      <td>5.614808</td>\n",
              "      <td>52.463305</td>\n",
              "      <td>53.459291</td>\n",
              "      <td>38.960644</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.282616</td>\n",
              "      <td>...</td>\n",
              "      <td>35.446952</td>\n",
              "      <td>37.679391</td>\n",
              "      <td>19.006272</td>\n",
              "      <td>30.825487</td>\n",
              "      <td>47.902400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.770235</td>\n",
              "      <td>51.443002</td>\n",
              "      <td>92.371679</td>\n",
              "      <td>63.034177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>South Dakota,2011</td>\n",
              "      <td>South Dakota</td>\n",
              "      <td>2011</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5.913704</td>\n",
              "      <td>67.656172</td>\n",
              "      <td>65.015726</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38.755585</td>\n",
              "      <td>...</td>\n",
              "      <td>42.802329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>35.381513</td>\n",
              "      <td>38.156667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>79.234963</td>\n",
              "      <td>51.126943</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>70.040519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>New Mexico,2011</td>\n",
              "      <td>New Mexico</td>\n",
              "      <td>2011</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.505855</td>\n",
              "      <td>59.988472</td>\n",
              "      <td>60.371746</td>\n",
              "      <td>44.485883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>34.527703</td>\n",
              "      <td>...</td>\n",
              "      <td>31.700018</td>\n",
              "      <td>9.800002</td>\n",
              "      <td>37.512929</td>\n",
              "      <td>31.054514</td>\n",
              "      <td>39.128370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>72.882793</td>\n",
              "      <td>44.066556</td>\n",
              "      <td>96.353161</td>\n",
              "      <td>67.954206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>Michigan,2014</td>\n",
              "      <td>Michigan</td>\n",
              "      <td>2014</td>\n",
              "      <td>10.3</td>\n",
              "      <td>4.558037</td>\n",
              "      <td>46.278891</td>\n",
              "      <td>45.439103</td>\n",
              "      <td>39.487153</td>\n",
              "      <td>31.401298</td>\n",
              "      <td>22.121860</td>\n",
              "      <td>...</td>\n",
              "      <td>20.964773</td>\n",
              "      <td>26.326637</td>\n",
              "      <td>28.509873</td>\n",
              "      <td>25.846464</td>\n",
              "      <td>49.564624</td>\n",
              "      <td>120.194129</td>\n",
              "      <td>49.262921</td>\n",
              "      <td>48.816906</td>\n",
              "      <td>94.585621</td>\n",
              "      <td>60.159225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>Oregon,2014</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>2014</td>\n",
              "      <td>8.9</td>\n",
              "      <td>2.506920</td>\n",
              "      <td>23.139446</td>\n",
              "      <td>29.480003</td>\n",
              "      <td>17.238046</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.252795</td>\n",
              "      <td>...</td>\n",
              "      <td>11.079828</td>\n",
              "      <td>10.911698</td>\n",
              "      <td>20.826361</td>\n",
              "      <td>11.487317</td>\n",
              "      <td>20.408963</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.393007</td>\n",
              "      <td>23.188030</td>\n",
              "      <td>47.901731</td>\n",
              "      <td>29.558001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>Arizona,2013</td>\n",
              "      <td>Arizona</td>\n",
              "      <td>2013</td>\n",
              "      <td>10.6</td>\n",
              "      <td>3.290547</td>\n",
              "      <td>48.940443</td>\n",
              "      <td>47.204166</td>\n",
              "      <td>30.924915</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.002291</td>\n",
              "      <td>...</td>\n",
              "      <td>23.842140</td>\n",
              "      <td>27.110506</td>\n",
              "      <td>28.579189</td>\n",
              "      <td>24.165763</td>\n",
              "      <td>43.040883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>50.488766</td>\n",
              "      <td>44.172483</td>\n",
              "      <td>89.119765</td>\n",
              "      <td>56.819195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Texas,2011</td>\n",
              "      <td>Texas</td>\n",
              "      <td>2011</td>\n",
              "      <td>10.2</td>\n",
              "      <td>3.411752</td>\n",
              "      <td>47.133800</td>\n",
              "      <td>43.564960</td>\n",
              "      <td>27.803677</td>\n",
              "      <td>32.117923</td>\n",
              "      <td>19.730116</td>\n",
              "      <td>...</td>\n",
              "      <td>20.597708</td>\n",
              "      <td>28.823534</td>\n",
              "      <td>17.477615</td>\n",
              "      <td>22.977555</td>\n",
              "      <td>37.361283</td>\n",
              "      <td>99.183684</td>\n",
              "      <td>58.172504</td>\n",
              "      <td>29.945781</td>\n",
              "      <td>63.636230</td>\n",
              "      <td>42.620401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Washington,2011</td>\n",
              "      <td>Washington</td>\n",
              "      <td>2011</td>\n",
              "      <td>8.9</td>\n",
              "      <td>5.686254</td>\n",
              "      <td>45.780676</td>\n",
              "      <td>43.343818</td>\n",
              "      <td>24.352186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.616175</td>\n",
              "      <td>...</td>\n",
              "      <td>23.811535</td>\n",
              "      <td>23.443141</td>\n",
              "      <td>25.363855</td>\n",
              "      <td>25.762713</td>\n",
              "      <td>32.312461</td>\n",
              "      <td>51.413583</td>\n",
              "      <td>48.477087</td>\n",
              "      <td>36.032322</td>\n",
              "      <td>70.826764</td>\n",
              "      <td>46.196938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>Illinois,2015</td>\n",
              "      <td>Illinois</td>\n",
              "      <td>2015</td>\n",
              "      <td>9.9</td>\n",
              "      <td>3.858192</td>\n",
              "      <td>45.960526</td>\n",
              "      <td>47.365028</td>\n",
              "      <td>29.006354</td>\n",
              "      <td>44.956067</td>\n",
              "      <td>22.365659</td>\n",
              "      <td>...</td>\n",
              "      <td>18.498807</td>\n",
              "      <td>32.214281</td>\n",
              "      <td>57.382783</td>\n",
              "      <td>21.239013</td>\n",
              "      <td>48.629034</td>\n",
              "      <td>116.491699</td>\n",
              "      <td>46.316835</td>\n",
              "      <td>52.612292</td>\n",
              "      <td>94.011899</td>\n",
              "      <td>70.665353</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Id        states  year  outcome         A          B  \\\n",
              "49        Wyoming,2010       Wyoming  2010      7.2       NaN        NaN   \n",
              "114          Iowa,2012          Iowa  2012      9.7  5.614808  52.463305   \n",
              "90   South Dakota,2011  South Dakota  2011      9.4  5.913704  67.656172   \n",
              "80     New Mexico,2011    New Mexico  2011     10.0  7.505855  59.988472   \n",
              "221      Michigan,2014      Michigan  2014     10.3  4.558037  46.278891   \n",
              "236        Oregon,2014        Oregon  2014      8.9  2.506920  23.139446   \n",
              "152       Arizona,2013       Arizona  2013     10.6  3.290547  48.940443   \n",
              "92          Texas,2011         Texas  2011     10.2  3.411752  47.133800   \n",
              "96     Washington,2011    Washington  2011      8.9  5.686254  45.780676   \n",
              "262      Illinois,2015      Illinois  2015      9.9  3.858192  45.960526   \n",
              "\n",
              "             C          D          E          F  ...          T          U  \\\n",
              "49         NaN        NaN        NaN        NaN  ...        NaN        NaN   \n",
              "114  53.459291  38.960644   0.000000  30.282616  ...  35.446952  37.679391   \n",
              "90   65.015726   0.000000   0.000000  38.755585  ...  42.802329   0.000000   \n",
              "80   60.371746  44.485883   0.000000  34.527703  ...  31.700018   9.800002   \n",
              "221  45.439103  39.487153  31.401298  22.121860  ...  20.964773  26.326637   \n",
              "236  29.480003  17.238046   0.000000  16.252795  ...  11.079828  10.911698   \n",
              "152  47.204166  30.924915   0.000000  22.002291  ...  23.842140  27.110506   \n",
              "92   43.564960  27.803677  32.117923  19.730116  ...  20.597708  28.823534   \n",
              "96   43.343818  24.352186   0.000000  17.616175  ...  23.811535  23.443141   \n",
              "262  47.365028  29.006354  44.956067  22.365659  ...  18.498807  32.214281   \n",
              "\n",
              "             V          W          X           Y          Z         AA  \\\n",
              "49         NaN        NaN        NaN         NaN        NaN        NaN   \n",
              "114  19.006272  30.825487  47.902400    0.000000  59.770235  51.443002   \n",
              "90   35.381513  38.156667   0.000000    0.000000  79.234963  51.126943   \n",
              "80   37.512929  31.054514  39.128370    0.000000  72.882793  44.066556   \n",
              "221  28.509873  25.846464  49.564624  120.194129  49.262921  48.816906   \n",
              "236  20.826361  11.487317  20.408963    0.000000  23.393007  23.188030   \n",
              "152  28.579189  24.165763  43.040883    0.000000  50.488766  44.172483   \n",
              "92   17.477615  22.977555  37.361283   99.183684  58.172504  29.945781   \n",
              "96   25.363855  25.762713  32.312461   51.413583  48.477087  36.032322   \n",
              "262  57.382783  21.239013  48.629034  116.491699  46.316835  52.612292   \n",
              "\n",
              "            AB         AC  \n",
              "49         NaN        NaN  \n",
              "114  92.371679  63.034177  \n",
              "90    0.000000  70.040519  \n",
              "80   96.353161  67.954206  \n",
              "221  94.585621  60.159225  \n",
              "236  47.901731  29.558001  \n",
              "152  89.119765  56.819195  \n",
              "92   63.636230  42.620401  \n",
              "96   70.826764  46.196938  \n",
              "262  94.011899  70.665353  \n",
              "\n",
              "[10 rows x 33 columns]"
            ]
          },
          "execution_count": 463,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the training data\n",
        "train_data = pd.read_csv('us_train.csv')\n",
        "\n",
        "# Display a sample of the training data\n",
        "train_data.sample(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Impute missing values (mainly found for all states in Y2010)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>states</th>\n",
              "      <th>year</th>\n",
              "      <th>outcome</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>...</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>AA</th>\n",
              "      <th>AB</th>\n",
              "      <th>AC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alabama,2010</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2010</td>\n",
              "      <td>13.1</td>\n",
              "      <td>4.977296</td>\n",
              "      <td>56.006209</td>\n",
              "      <td>53.386040</td>\n",
              "      <td>40.943280</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.966852</td>\n",
              "      <td>...</td>\n",
              "      <td>26.299833</td>\n",
              "      <td>38.192169</td>\n",
              "      <td>29.174189</td>\n",
              "      <td>31.091563</td>\n",
              "      <td>51.739476</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.782957</td>\n",
              "      <td>55.159269</td>\n",
              "      <td>100.995312</td>\n",
              "      <td>71.881987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alabama,2011</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2011</td>\n",
              "      <td>11.7</td>\n",
              "      <td>5.686254</td>\n",
              "      <td>55.929102</td>\n",
              "      <td>53.958630</td>\n",
              "      <td>39.308647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.419012</td>\n",
              "      <td>...</td>\n",
              "      <td>31.846101</td>\n",
              "      <td>31.513731</td>\n",
              "      <td>19.822173</td>\n",
              "      <td>31.054514</td>\n",
              "      <td>43.672310</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.226040</td>\n",
              "      <td>47.718480</td>\n",
              "      <td>84.129253</td>\n",
              "      <td>60.503087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alabama,2012</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2012</td>\n",
              "      <td>12.2</td>\n",
              "      <td>5.182900</td>\n",
              "      <td>54.726428</td>\n",
              "      <td>53.459291</td>\n",
              "      <td>39.861468</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.437205</td>\n",
              "      <td>...</td>\n",
              "      <td>28.927972</td>\n",
              "      <td>29.658531</td>\n",
              "      <td>19.006272</td>\n",
              "      <td>35.567869</td>\n",
              "      <td>56.714634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.583259</td>\n",
              "      <td>50.424329</td>\n",
              "      <td>90.950577</td>\n",
              "      <td>65.401658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alabama,2013</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2013</td>\n",
              "      <td>13.8</td>\n",
              "      <td>4.730162</td>\n",
              "      <td>54.839514</td>\n",
              "      <td>52.374146</td>\n",
              "      <td>46.201077</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.890228</td>\n",
              "      <td>...</td>\n",
              "      <td>26.213871</td>\n",
              "      <td>42.245818</td>\n",
              "      <td>20.162031</td>\n",
              "      <td>32.176514</td>\n",
              "      <td>47.926604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.504497</td>\n",
              "      <td>54.301205</td>\n",
              "      <td>100.949823</td>\n",
              "      <td>68.126497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alabama,2014</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2014</td>\n",
              "      <td>12.9</td>\n",
              "      <td>4.785939</td>\n",
              "      <td>55.831873</td>\n",
              "      <td>52.975344</td>\n",
              "      <td>35.277862</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.947280</td>\n",
              "      <td>...</td>\n",
              "      <td>23.354540</td>\n",
              "      <td>45.552010</td>\n",
              "      <td>33.160420</td>\n",
              "      <td>29.981898</td>\n",
              "      <td>52.480190</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.849149</td>\n",
              "      <td>57.054759</td>\n",
              "      <td>104.734293</td>\n",
              "      <td>72.677907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Alabama,2015</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>2015</td>\n",
              "      <td>13.5</td>\n",
              "      <td>4.501224</td>\n",
              "      <td>58.704126</td>\n",
              "      <td>54.162787</td>\n",
              "      <td>44.067346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.140534</td>\n",
              "      <td>...</td>\n",
              "      <td>21.156681</td>\n",
              "      <td>41.990755</td>\n",
              "      <td>53.720052</td>\n",
              "      <td>26.677021</td>\n",
              "      <td>57.903644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>62.751841</td>\n",
              "      <td>66.297570</td>\n",
              "      <td>124.212613</td>\n",
              "      <td>92.700786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Alaska,2010</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2010</td>\n",
              "      <td>5.3</td>\n",
              "      <td>3.370060</td>\n",
              "      <td>46.371256</td>\n",
              "      <td>55.469361</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.103201</td>\n",
              "      <td>...</td>\n",
              "      <td>24.480612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>47.988532</td>\n",
              "      <td>25.029107</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>57.782747</td>\n",
              "      <td>45.811168</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>68.252144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Alaska,2011</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2011</td>\n",
              "      <td>7.9</td>\n",
              "      <td>4.549003</td>\n",
              "      <td>50.967649</td>\n",
              "      <td>54.400914</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.082439</td>\n",
              "      <td>...</td>\n",
              "      <td>27.171444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.068989</td>\n",
              "      <td>28.687129</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.811412</td>\n",
              "      <td>37.736553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>58.714818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Alaska,2012</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2012</td>\n",
              "      <td>6.9</td>\n",
              "      <td>3.239312</td>\n",
              "      <td>40.941952</td>\n",
              "      <td>53.676605</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.785029</td>\n",
              "      <td>...</td>\n",
              "      <td>26.075919</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.238747</td>\n",
              "      <td>25.334307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>70.177988</td>\n",
              "      <td>48.896319</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>61.554502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Alaska,2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2013</td>\n",
              "      <td>7.1</td>\n",
              "      <td>3.084888</td>\n",
              "      <td>50.251347</td>\n",
              "      <td>55.745873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.438540</td>\n",
              "      <td>...</td>\n",
              "      <td>25.714560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43.260280</td>\n",
              "      <td>24.165763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.996632</td>\n",
              "      <td>45.579250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.147719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Id   states  year  outcome         A          B          C  \\\n",
              "0  Alabama,2010  Alabama  2010     13.1  4.977296  56.006209  53.386040   \n",
              "1  Alabama,2011  Alabama  2011     11.7  5.686254  55.929102  53.958630   \n",
              "2  Alabama,2012  Alabama  2012     12.2  5.182900  54.726428  53.459291   \n",
              "3  Alabama,2013  Alabama  2013     13.8  4.730162  54.839514  52.374146   \n",
              "4  Alabama,2014  Alabama  2014     12.9  4.785939  55.831873  52.975344   \n",
              "5  Alabama,2015  Alabama  2015     13.5  4.501224  58.704126  54.162787   \n",
              "6   Alaska,2010   Alaska  2010      5.3  3.370060  46.371256  55.469361   \n",
              "7   Alaska,2011   Alaska  2011      7.9  4.549003  50.967649  54.400914   \n",
              "8   Alaska,2012   Alaska  2012      6.9  3.239312  40.941952  53.676605   \n",
              "9   Alaska,2013   Alaska  2013      7.1  3.084888  50.251347  55.745873   \n",
              "\n",
              "           D    E          F  ...          T          U          V          W  \\\n",
              "0  40.943280  0.0  29.966852  ...  26.299833  38.192169  29.174189  31.091563   \n",
              "1  39.308647  0.0  29.419012  ...  31.846101  31.513731  19.822173  31.054514   \n",
              "2  39.861468  0.0  26.437205  ...  28.927972  29.658531  19.006272  35.567869   \n",
              "3  46.201077  0.0  27.890228  ...  26.213871  42.245818  20.162031  32.176514   \n",
              "4  35.277862  0.0  29.947280  ...  23.354540  45.552010  33.160420  29.981898   \n",
              "5  44.067346  0.0  36.140534  ...  21.156681  41.990755  53.720052  26.677021   \n",
              "6   0.000000  0.0  22.103201  ...  24.480612   0.000000  47.988532  25.029107   \n",
              "7   0.000000  0.0  20.082439  ...  27.171444   0.000000  27.068989  28.687129   \n",
              "8   0.000000  0.0  17.785029  ...  26.075919   0.000000  26.238747  25.334307   \n",
              "9   0.000000  0.0  18.438540  ...  25.714560   0.000000  43.260280  24.165763   \n",
              "\n",
              "           X    Y          Z         AA          AB         AC  \n",
              "0  51.739476  0.0  66.782957  55.159269  100.995312  71.881987  \n",
              "1  43.672310  0.0  76.226040  47.718480   84.129253  60.503087  \n",
              "2  56.714634  0.0  69.583259  50.424329   90.950577  65.401658  \n",
              "3  47.926604  0.0  61.504497  54.301205  100.949823  68.126497  \n",
              "4  52.480190  0.0  63.849149  57.054759  104.734293  72.677907  \n",
              "5  57.903644  0.0  62.751841  66.297570  124.212613  92.700786  \n",
              "6   0.000000  0.0  57.782747  45.811168    0.000000  68.252144  \n",
              "7   0.000000  0.0  48.811412  37.736553    0.000000  58.714818  \n",
              "8   0.000000  0.0  70.177988  48.896319    0.000000  61.554502  \n",
              "9   0.000000  0.0  55.996632  45.579250    0.000000  66.147719  \n",
              "\n",
              "[10 rows x 33 columns]"
            ]
          },
          "execution_count": 464,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a function to fill NaNs with mean for each column in a group\n",
        "def fill_nans(group, imputation_func):\n",
        "    return group.fillna(imputation_func)\n",
        "\n",
        "# Apply the function to each group (each state)\n",
        "train_data = train_data.groupby('states', group_keys=True).apply(lambda group: fill_nans(group, group.mean()))\n",
        "\n",
        "# Reset the index of the dataframe\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "train_data.head(10)\n",
        "# train_data[train_data['states'] == 'Alabama']\n",
        "# train_data[train_data['states'] == 'Alaska']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = train_data.drop(['Id', 'states', 'year', 'outcome'], axis=1)\n",
        "y = train_data['outcome']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale X_train and X_test\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Scale Y_train and Y_test\n",
        "scaler = StandardScaler()\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Linear Regression (No regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average MAE for Linear Regression: 0.8534117010232437\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores_linear = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Calculate the average MAE\n",
        "avg_mae_linear = np.mean(-cv_scores_linear)\n",
        "print(f\"Average MAE for Linear Regression: {avg_mae_linear}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE on Test Data: 0.7120501563092623\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Train the model on the entire training dataset\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "y_pred_test = linear_model.predict(X_test)\n",
        "\n",
        "# Calculate the MAE on the test dataset\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "print(f\"MAE on Test Data: {mae_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Linear regression (Ridge and Lasso regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'alpha': 100.0}\n",
            "Best MAE from CV: 0.7283418293444243\n",
            "MAE on Test Data: 0.7084924097437492\n"
          ]
        }
      ],
      "source": [
        "# GridSearchCV for Ridge Regression\n",
        "alphas = np.linspace(0.01, 100, 10000)\n",
        "parameters = {'alpha': alphas}\n",
        "ridge = Ridge()\n",
        "ridge_grid = GridSearchCV(ridge, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and MAE from GridSearchCV\n",
        "print(\"Best parameters:\", ridge_grid.best_params_)\n",
        "print(\"Best MAE from CV:\", -ridge_grid.best_score_)\n",
        "\n",
        "# Train Ridge model with best alpha\n",
        "best_alpha = ridge_grid.best_params_['alpha']\n",
        "ridge_best = Ridge(alpha=best_alpha)\n",
        "ridge_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred_test = ridge_best.predict(X_test)\n",
        "\n",
        "# Calculate MAE on test data\n",
        "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
        "print(f\"MAE on Test Data: {mae_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters (scaled data): {'alpha': 9.14}\n",
            "Best MAE (scaled data): 0.4222854011905584\n",
            "MAE on Scaled Test Data: 0.41954266212252433\n"
          ]
        }
      ],
      "source": [
        "# Initialize Ridge and GridSearchCV\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': np.linspace(0.01, 100, 10000)}\n",
        "ridge_grid_scaled = GridSearchCV(ridge, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "ridge_grid_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Best parameters and MAE\n",
        "print(\"Best parameters (scaled data):\", ridge_grid_scaled.best_params_)\n",
        "print(\"Best MAE (scaled data):\", -ridge_grid_scaled.best_score_)\n",
        "\n",
        "# Train Ridge model with best alpha on scaled training data\n",
        "best_alpha = ridge_grid.best_params_['alpha']\n",
        "ridge_best_scaled = Ridge(alpha=best_alpha)\n",
        "ridge_best_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Make predictions on scaled test data\n",
        "y_pred_test_scaled = ridge_best_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate MAE on scaled test data\n",
        "mae_test_scaled = mean_absolute_error(y_test_scaled, y_pred_test_scaled)\n",
        "print(f\"MAE on Scaled Test Data: {mae_test_scaled}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters on unscaled data: 0.07660606060606061\n",
            "Best MAE on unscaled data: 0.722297109385398\n",
            "MAE on unscaled test data: 0.7126673475262825\n"
          ]
        }
      ],
      "source": [
        "# Define a range of alpha values for Lasso Regression\n",
        "parameters = {'alpha': np.linspace(0.001, 0.5, 100)}\n",
        "\n",
        "lasso = Lasso()\n",
        "lasso_grid = GridSearchCV(lasso, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and MAE from GridSearchCV\n",
        "best_alpha_unscaled = lasso_grid.best_params_['alpha']\n",
        "print(\"Best parameters on unscaled data:\", best_alpha_unscaled)\n",
        "print(\"Best MAE on unscaled data:\", -lasso_grid.best_score_)\n",
        "\n",
        "# Retrain the model with the best alpha and evaluate on the test set\n",
        "lasso_best_unscaled = Lasso(alpha=best_alpha_unscaled)\n",
        "lasso_best_unscaled.fit(X_train, y_train)\n",
        "y_pred_test_unscaled = lasso_best_unscaled.predict(X_test)\n",
        "mae_test_unscaled = mean_absolute_error(y_test, y_pred_test_unscaled)\n",
        "print(\"MAE on unscaled test data:\", mae_test_unscaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters on scaled data: 0.0060404040404040404\n",
            "Best MAE on scaled data: 0.42915095594377595\n",
            "MAE on scaled test data: 0.4214905193939588\n"
          ]
        }
      ],
      "source": [
        "# GridSearchCV with scaled data\n",
        "lasso_grid_scaled = GridSearchCV(Lasso(), parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "lasso_grid_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Best parameters and MAE from GridSearchCV\n",
        "best_alpha_scaled = lasso_grid_scaled.best_params_['alpha']\n",
        "print(\"Best parameters on scaled data:\", best_alpha_scaled)\n",
        "print(\"Best MAE on scaled data:\", -lasso_grid_scaled.best_score_)\n",
        "\n",
        "# Retrain the model with the best alpha and evaluate on the scaled test set\n",
        "lasso_best_scaled = Lasso(alpha=best_alpha_scaled)\n",
        "lasso_best_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred_test_scaled = lasso_best_scaled.predict(X_test_scaled)\n",
        "mae_test_scaled = mean_absolute_error(y_test_scaled, y_pred_test_scaled)\n",
        "print(\"MAE on scaled test data:\", mae_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Polynomial Regression (No regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 470,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average MAE for Polynomial Regression: 7.414024065889262\n"
          ]
        }
      ],
      "source": [
        "# Polynomial Regression\n",
        "poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "cv_scores_poly = cross_val_score(poly_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Calculate the average MAE\n",
        "avg_mae_poly = np.mean(-cv_scores_poly)\n",
        "print(f\"Average MAE for Polynomial Regression: {avg_mae_poly}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Polynomial regression (Ridge regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'ridge__alpha': 0.5}\n",
            "Best MAE: 2.429973694681771\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# Define a pipeline combining Polynomial Features with Ridge Regression\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('ridge', Ridge())\n",
        "])\n",
        "\n",
        "parameters = {'ridge__alpha': np.linspace(0.001,0.5,100)}\n",
        "poly_ridge_grid = GridSearchCV(pipeline, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "poly_ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", poly_ridge_grid.best_params_)\n",
        "print(\"Best MAE:\", -poly_ridge_grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Polynomial regression (Lasso regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters on unscaled data: 0.5\n",
            "Best MAE on unscaled data: 0.8971506140335519\n",
            "MAE on unscaled test data: 0.8712613998440097\n"
          ]
        }
      ],
      "source": [
        "# Define a pipeline combining Polynomial Features with Lasso Regression\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('lasso', Lasso())\n",
        "])\n",
        "\n",
        "parameters = {'lasso__alpha': np.linspace(0.001, 0.5, 100)}\n",
        "poly_lasso_grid = GridSearchCV(pipeline, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "poly_lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and MAE from GridSearchCV\n",
        "best_alpha_poly_unscaled = poly_lasso_grid.best_params_['lasso__alpha']\n",
        "print(\"Best parameters on unscaled data:\", best_alpha_poly_unscaled)\n",
        "print(\"Best MAE on unscaled data:\", -poly_lasso_grid.best_score_)\n",
        "\n",
        "# Retrain the model with the best parameters and evaluate on the test set\n",
        "pipeline_best_unscaled = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('lasso', Lasso(alpha=best_alpha_poly_unscaled))\n",
        "])\n",
        "pipeline_best_unscaled.fit(X_train, y_train)\n",
        "y_pred_test_unscaled = pipeline_best_unscaled.predict(X_test)\n",
        "mae_test_unscaled = mean_absolute_error(y_test, y_pred_test_unscaled)\n",
        "print(\"MAE on unscaled test data:\", mae_test_unscaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters on scaled data: 0.0060404040404040404\n",
            "Best MAE on scaled data: 0.4036970883205181\n",
            "MAE on scaled test data: 0.4439842263136876\n"
          ]
        }
      ],
      "source": [
        "# GridSearchCV with scaled data\n",
        "poly_lasso_grid_scaled = GridSearchCV(pipeline, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "poly_lasso_grid_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Best parameters and MAE from GridSearchCV\n",
        "best_alpha_poly_scaled = poly_lasso_grid_scaled.best_params_['lasso__alpha']\n",
        "print(\"Best parameters on scaled data:\", best_alpha_poly_scaled)\n",
        "print(\"Best MAE on scaled data:\", -poly_lasso_grid_scaled.best_score_)\n",
        "\n",
        "# Retrain the model with the best parameters and evaluate on the scaled test set\n",
        "pipeline_best_scaled = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('lasso', Lasso(alpha=best_alpha_poly_scaled))\n",
        "])\n",
        "pipeline_best_scaled.fit(X_train_scaled, y_train_scaled)\n",
        "y_pred_test_scaled = pipeline_best_scaled.predict(X_test_scaled)\n",
        "mae_test_scaled = mean_absolute_error(y_test_scaled, y_pred_test_scaled)\n",
        "print(\"MAE on scaled test data:\", mae_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### kNN Regression (No optimization via GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average MAE for kNN (3 neighbors): 1.2387777777777778\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Trying kNN with 3 neighbors as an example\n",
        "knn_model = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores_knn = cross_val_score(knn_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Calculate the average MAE\n",
        "avg_mae_knn = np.mean(-cv_scores_knn)\n",
        "print(f\"Average MAE for kNN (3 neighbors): {avg_mae_knn}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### kNN Regression (Optimization via GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'n_neighbors': 3}\n",
            "Best MAE: 1.042638888888889\n"
          ]
        }
      ],
      "source": [
        "parameters = {'n_neighbors': range(1, 51, 2)}\n",
        "\n",
        "knn = KNeighborsRegressor()\n",
        "knn_grid = GridSearchCV(knn, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
        "knn_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", knn_grid.best_params_)\n",
        "print(\"Best MAE:\", -knn_grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Best model: Linear Regression (lasso regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare the us_test_x dataset\n",
        "test_data = pd.read_csv('us_test_x.csv')\n",
        "us_test_x = test_data.drop(['Id', 'states', 'year'], axis=1)  # Adjust the columns as needed\n",
        "\n",
        "# Scale the features of us_test_x using the same scaler as the training data\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)  # Assuming X_train is your training data\n",
        "us_test_x_scaled = scaler_X.transform(us_test_x)\n",
        "\n",
        "# Best alpha from the Lasso regression with polynomial features\n",
        "best_alpha_poly_scaled = poly_lasso_grid_scaled.best_params_['lasso__alpha']\n",
        "\n",
        "# Retrain the model with the best alpha from polynomial Lasso regression\n",
        "pipeline_best_scaled = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2)),\n",
        "    ('lasso', Lasso(alpha=best_alpha_poly_scaled))\n",
        "])\n",
        "pipeline_best_scaled.fit(X_train_scaled, y_train_scaled)  # Assuming y_train_scaled is your scaled target variable\n",
        "\n",
        "# Make predictions on the scaled us_test_x data\n",
        "us_test_x_predictions = pipeline_best_scaled.predict(us_test_x_scaled)\n",
        "\n",
        "# Undo the scaling\n",
        "us_test_x_predictions = scaler.inverse_transform(us_test_x_predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_data['Id'],  # Ensure this matches the identifier column in your dataset\n",
        "    'Predicted': us_test_x_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV file for submission\n",
        "submission.to_csv('us_predictions.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoSZ0d_jaBYH"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +20 points for logical and reasonable steps to training and testing the models using the techniques taught in the course\n",
        "- +15 points showing code and evaluation of **at least two regression models** at least one of which makes the same predictions as submitted on Kaggle and in the document `us_predictions.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQkC-_qeYvnL"
      },
      "source": [
        "## B. Kaggle Submission (35 points)\n",
        "\n",
        "Create an account on Kaggle, and submit your predictions as `us_predictions.csv` with the two columns `Id` and `Predicted` to Kaggle.\n",
        "\n",
        "You will be evaluated on the `Mean Absolute Error` as a scoring metric.\n",
        "\n",
        "There are seven benchmarks/baselines that we have provided you on Kaggle. These are as follows:\n",
        "\n",
        "- `Trivial Baseline`\n",
        "- `Baseline A (1 and 2)`\n",
        "- `Baseline B (1 and 2)`\n",
        "- `Baseline C (1 and 2)`\n",
        "\n",
        "To be able to get full points on this task, you would need to pass the `Trivial Baseline`, either of `A1` or `A2`, either of `B1` or `B2`, **and** either of `C1` or `C2` baseline.\n",
        "\n",
        "Note that the score you see on Kaggle Leaderboard for your submission is only based on 50% of the test dataset (i.e. 25 data points) -- we have hidden the other 50% of the dataset, and your score on those will only be revealed once the competition ends. In general, if you pass the baseline on the publicly available data, your model should pass the baselines on the hidden data as well. But we have kept it hidden so that you don't overfit your model on the test set.\n",
        "\n",
        "The Kaggle data points for the test set are from 2016 the features for which are provided in `us_test_x.csv`.\n",
        "\n",
        "You have a maximum for 15 submissions per day on Kaggle. Before submitting the notebook, enter your Kaggle username in the **Kaggle Username** section above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH5VV_UXWFZe"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +10 points for achieving/crossing Baseline A1 or Baseline A2 across both public (7 points) and hidden data points (3 points).\n",
        "- +5 points for achieving/crossing Trivial Baseline across both public (3 points) and hidden data points (2 points).\n",
        "- +10 points for achieving/crossing Baseline B1 or Baseline B2 across both public (7 points) and hidden data points (3 points).\n",
        "- +10 points for achieving/crossing Baseline C1 or Baseline C2 across both public (7 points) and hidden data points (3 points).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGyqP7dnRhU"
      },
      "source": [
        "## *Concepts required to complete this task*\n",
        "\n",
        "*   Basics of Machine Learning\n",
        "*   Basics of Regression\n",
        "*   Feature Engineering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuRYqcxmn4vS"
      },
      "source": [
        "# Part II: Transfer Learning (30 points)\n",
        "\n",
        "Many machine learning methods work well only under a common assumption: the training and test data are drawn from the same feature space and/or the same distribution. When the distribution changes, most statistical models need to be rebuilt from scratch using newly collected training data. In many real world applications, it is expensive or impossible to recollect the needed training data and rebuild the models. It would be nice to reduce the need and effort to recollect the training data. In such cases, **knowledge transfer** or **transfer learning** between task domains would be desirable.\n",
        "\n",
        "**Transfer learning** is a machine learning method where a model developed for a task is reused for a model on a second task. For example, in the paper on *Revealing Inherent Gender Biases in Using Word Embeddings for Sentiment Analysis* in PS7, the (imaginary) authors used word embeddings for sentiment analysis. That was a transfer learning approach where word embeddings were created from a machine learning model that was trained for the purpose of [predicting words](https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/#:~:text=Language%20modeling%20involves%20predicting%20the,machine%20translation%20and%20speech%20recognition.), but the model was later **reused** to extract word embeddings to be used for the sentiment analysis task. Another example would be a **spam filtering model** that has been trained on emails of one user (the source distribution) and is applied to a new user who receives significantly different emails (the target distribution). This process of applying the model to a different target distribution is sometimes also known as **domain adaptation**. <Sup>2</Sup>\n",
        "\n",
        "---------\n",
        "<sup> 2. Some people distinguish between **transfer learning** and **domain adaptation**, some don't. These are not very precisely defined terms in the literature.</sup>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzpsVzu8ovqK"
      },
      "source": [
        "## A. Cross-Country Generalizability of the Model (15 points)\n",
        "\n",
        "Using a model trained on all the data from the United States, estimate the prevalence of Climate Change Denialism disease for the 8 provinces of the **Dominion of Canada** for the years 2011 to 2014. You may have to modify and retrain your model according to the data and features available to you for Canada.\n",
        "\n",
        "The dataset (i.e. features) for Canada is available as `ca_test_x.csv`. You will submit your final predictions as `ca_predictions.csv` the template for which is provided to you in the handout. As you will notice, the features for Canada are a subset of the features for the USA, therefore, you'll have to train your US based model accordingly.\n",
        "\n",
        "As a submission for this part, you will fill the `ca_predictions.csv` file and submit that along with this Notebook to Brightspace. You will also submit `ca_predictions.csv` file to Kaggle (see Part B)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {
        "id": "HJ6leqzmEDIg"
      },
      "outputs": [],
      "source": [
        "######### SOLUTION #########\n",
        "\n",
        "\n",
        "######### SOLUTION END #########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk6-XiEMFItX"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +5 points for logical and reasonable steps to training and testing the models using the techniques taught in the course\n",
        "- +10 points showing code and evaluation of **at least two regression models** at least one of which makes the same predictions as submitted on Kaggle and in the document `ca_predictions.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhewfxmLpQYj"
      },
      "source": [
        "## B. Kaggle Submission (15 points)\n",
        "\n",
        "You will submit your predictions as `ca_predictions.csv` with the two columns `Id` and `Predicted` to Kaggle.\n",
        "\n",
        "You will be evaluated on the `Mean Absolute Error` scoring metric.\n",
        "\n",
        "There is one benchmark/baseline that we have provided you on Kaggle that you will have to meet/beat to receive all the points.\n",
        "\n",
        "Note that the score you see on Kaggle Leaderboard for your submission is only based on 75% of the dataset (i.e. 24 data points) -- we have hidden 25% of the dataset (8 data points), and your score on those will only be revealed once the competition ends.\n",
        "\n",
        "You have a maximum for 15 submissions per day on Kaggle. Before submitting the notebook, enter your Kaggle username in the **Kaggle Username** section above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRVWGA6-EmVg"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +15 points for achieving/crossing the baseline provided across both public (10 points) and hidden data points (5 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N08hHBwwoHHT"
      },
      "source": [
        "## *Concepts required to complete this task*\n",
        "\n",
        "*   Basics of Machine Learning\n",
        "*   Basics of Regression\n",
        "*   Feature Engineering\n",
        "*   Concept of Transfer Learning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUxdctIdxU6B"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
