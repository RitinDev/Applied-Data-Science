{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxdctIdxU6B"
      },
      "source": [
        "---\n",
        "---\n",
        "Problem Set 5: Pandas II (Data Manipulation)\n",
        "\n",
        "Applied Data Science using Python\n",
        "\n",
        "New York University, Abu Dhabi\n",
        "\n",
        "Out: 5th Oct 2023 || **Due: 17th Oct 2023 at 23:59**\n",
        "\n",
        "---\n",
        "---\n",
        "#Start Here\n",
        "## Learning Goals\n",
        "### General Goals\n",
        "- Understand Data Manipulation using Pandas\n",
        "- Bring together all the different concepts learned in the class\n",
        "- Understand the challenges of data cleaning\n",
        "\n",
        "### Specific Goals\n",
        "- Understand Merging and Concatenating\n",
        "- Be comfortable with multi-level indexing\n",
        "- Understand How to save Pandas files as csv\n",
        "- Be able to effectively use various Pandas functions such as\n",
        "  - `replace()`\n",
        "  - `extract()`\n",
        "  - `apply()`\n",
        "  - `merge()`\n",
        "  - `concat()`\n",
        "  - `groupby()`\n",
        "  - `transform()`\n",
        "  - `agg()`\n",
        "  - `filter()`\n",
        "\n",
        "## Collaboration Policy\n",
        "- You are allowed to talk with / work with other students on homework assignments.\n",
        "- You can share ideas but not code, analyses or results; you must submit your own code and results. All submitted code will be compared against all code submitted this and previous semesters and online using MOSS. We will also critically analyze the similarities in the submitted reports, methodologies, and results, **but we will not police you**. We expect you all to be mature and responsible enough to finish your work with full integrity.\n",
        "- You are expected to comply with the [University Policy on Academic Integrity and Plagiarism](https://www.nyu.edu/about/policies-guidelines-compliance/policies-and-guidelines/academic-integrity-for-students-at-nyu.html). Violations may result in penalties, such as failure in a particular assignment.\n",
        "\n",
        "## Late Submission Policy\n",
        "You can submit the homework for upto 3 late days. However, we will deduct **20 points** from your homework grade **for each late day you take**. We will not accept the homework after 3 late days.\n",
        "\n",
        "## Distribution of Class Materials\n",
        "These problem sets and recitations are intellectual property of NYUAD, and we request the students to **not** distribute them or their solutions to other students who have not signed up for this class, and/or intend to sign up in the future. We also request you don't post these problem sets, and recitations online or on any public platforms.\n",
        "\n",
        "## Disclaimer\n",
        "The number of points do not necessarily signify/correlate to the difficulty level of the tasks.\n",
        "\n",
        "## Submission\n",
        "You will submit all your code as a Python Notebook through [Brightspace](https://brightspace.nyu.edu/) as **P5_YOUR NETID.ipynb**.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMDlF4mjG6v"
      },
      "source": [
        "# General Instructions\n",
        "This homework is worth 100 points. It has 3 parts. Below each part, we provide a set of concepts required to complete that part. All the parts need to be completed in this Jupyter (Colab) Notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_WVcCKLBmp5"
      },
      "source": [
        "# Exploring the Project Data-verse\n",
        "\n",
        "In your recitation, you used your Pandas knowledge on data manipulation to explore the **Suicide Rates Dataset**. In this homework, you will explore 3 more datasets using your *data wrangling* skills: **(i) Health Trends Dataset**, **(ii) MovieLens Dataset**, and **(iii) Coronavirus Pandemic (COVID-19) Dataset**.\n",
        "\n",
        "The recitation, and the homework for this week is meant to help you with your decision to choose the problem and the dataset for your project. By the end of this homework, you will have a good idea about the ins and outs of the three project datasets (Suicide Rates, Health Trends, MovieLens) to be able to choose and formulate an interesting hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWf22cCjQNY"
      },
      "source": [
        "# Part I: Health Trends Dataset: README (35 points)\n",
        "\n",
        "Typically when a dataset is released to the public, it comes with a **README** file. A README file contains the description, and detailed information about the different folders, files, and fields in the dataset, along with the information about licenses, credits, and citations. It may also contain information on how the data was collected, how many subjects were involved, and so on. A README file is an important part of the dataset as it documents its motivation, composition, collection process, recommended uses, and so on. Furthermore, it facilitates better communication between dataset creators and dataset consumers.\n",
        "\n",
        "In this part of the homework, you have been given a messy dataset with a clean README file. There is a discrepancy between the dataset we have provided you, and the attached README file as the README file **does not** describe the data. Instead, it describes the *cleaner* version of the dataset that we have hidden from you, and we have no intention of providing that to you. :)\n",
        "\n",
        "Instead, we want you to manipulate the messy dataset, and clean it so that it adheres to the README file provided. What this means is merging and/or concatenating the different files, removing redundant or unnecessary fields, dealing with NaNs, defining columns properly, sorting the data, and validating that the final dataset adheres to the README file completely.\n",
        "\n",
        "The messy dataset we have provided corresponds to the **\"Health Trends Dataset\"**. This dataset is collected from two sources: Google search data from **Google Trends**, and official health statistics from **CDC/BRFSS**. The dataset is for two outcome variables: **obesity** and **exercise**.\n",
        "\n",
        "There are 3 folders inside **health_trends/** directory that we provide you:\n",
        "\n",
        "1. **health_statistics/:** This directory contains 6 sub-directories, 3 corresponding to \"exercise\", and 3 corresponding to \"obesity\".\n",
        "  - **exercise_age/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise exercise related statistics for the U.S *stratified by age group*.\n",
        "  - **exercise_gender/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise exercise related statistics for the U.S *stratified by gender*.\n",
        "  - **exercise_overall/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the *overall* state-wise exercise related statistics for the U.S.\n",
        "  - **obesity_age/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise obesity related statistics for the U.S *stratified by age group*.\n",
        "  - **obesity_gender/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the state-wise obesity related statistics for the U.S *stratified by gender*.\n",
        "  - **obesity_overall/:** This directory contains 15 files, 1 for each year from 2004 to 2018. Each file contains the *overall* state-wise obesity related statistics for the U.S.\n",
        "\n",
        "2. **spatial_search_intensity/:** This directory contains 1215 files, 1 for each (year, keywords) pair. Since there are 81 search keywords, and 15 years, this is equal to 1215 files. Each file is named in the **\\<year\\>_spatial_\\<keyword\\>.csv** format.\n",
        "\n",
        "3. **temporal_search_intensity/:** This directory contains 81 files, 1 for each keyword. Each file is named in the **2004_2018_temporal_\\<keyword\\>.csv** format.\n",
        "\n",
        "In total, you have **1386** number of files that you need to clean and synthesize into just **3** clean output files.\n",
        "\n",
        "We have also provided you with the paths to the files in each of the 3 directories above in **stats_paths.txt**, **spatial_paths.txt**, **temporal_paths.txt**. This is so that you can read the csv files within each directory using these path files.\n",
        "\n",
        "But wait! We told you about the directory structure of the data provided. What is the specification of the output files? What is this data about? What is temporal data? Spatial data? What do we have to do exactly? I am so confused!\n",
        "\n",
        "Well, to reiterate, you have to convert the given data into 3 files only, such that each file adheres completely to the provided README file. Now, to understand the data, and be able to actually do this part, you will have to first read the README file inside the **health_trends/** directory.\n",
        "\n",
        "Don't get intimidated by this part. It may look like a tedious task. However, once you actually understand what needs to be done, and get a sense of the dataset, it is not really that hard.\n",
        "\n",
        "Tips: Once you have read this part including the prompt, go back and open at least one csv file from each directory and sub-directory, and get a feel of what the data looks like. Then read the README file and identify the logic for each output file -- one at a time.\n",
        "\n",
        "![readme](https://drive.google.com/uc?id=1XeplvYB0L82k0i4RpAjMvbn6Tnb9CJ-X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfGNVsfFlYHV"
      },
      "source": [
        "## Prompt\n",
        "\n",
        "More concretely, write a function `clean(spatial_paths, temporal_paths, stats_paths)` that takes in 3 lists of paths, and creates and saves three output files to the output directory. The output directory is called `preprocessed_data/` and the three output files should be named as `spatial_trends.csv`, `temporal_trends.csv` and `health_stats.csv`, each adhering to the fields and specifications provided in the README file.\n",
        "\n",
        "Notes:\n",
        "- You should read the README file before starting to code.\n",
        "- You should simulataneously inspect all the different types of files in the provided **health\\_trends** folder to understand what these files look like.\n",
        "- You must remove the rows which are empty and uninformative. These would be the empty rows at the end of most of the csv files.\n",
        "- Finally, you should visually inspect to validate that the final dataframes and the output csv files have the required columns and rows sorted as described in the README, and have the required shape. There should be no redundant/extra column(s) or row(s) in the output files, and the column names should match the fields described in the README file.\n",
        "- You should submit the three generated output files with your notebook. We will also use your code to generate these datasets on our own.\n",
        "- Because there are many files to process, your program will take some time (in minutes) to complete its execution. This is primarily because of the `read_csv()` function which is an [I/O bound](https://en.wikipedia.org/wiki/I/O_bound) process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9qY33nbA7d_",
        "outputId": "8def9cbf-203c-46f8-fe36-488df57513d0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Commenting these lines out since I run the Jupyter notebook locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "yW41yg9o_L-l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\"\"\"\n",
        "You can define the input directory as per your directory structure.\n",
        "We will define it to the directory that contains the health_trends folder.\n",
        "\"\"\"\n",
        "\n",
        "input_directory = \"health_trends/\"\n",
        "\n",
        "temporal_paths = list(map(lambda e:e.rstrip(),\n",
        "                          open(input_directory+\"temporal_paths.txt\").readlines()))\n",
        "\n",
        "spatial_paths = list(map(lambda e:e.rstrip(),\n",
        "                         open(input_directory+\"spatial_paths.txt\").readlines()))\n",
        "\n",
        "stats_paths = list(map(lambda e:e.rstrip(),\n",
        "                       open(input_directory+\"stats_paths.txt\").readlines()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "_mFPHOvKZxHO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['health_statistics/obesity_age/2008.csv',\n",
              " 'health_statistics/obesity_age/2009.csv',\n",
              " 'health_statistics/obesity_age/2018.csv',\n",
              " 'health_statistics/obesity_age/2015.csv',\n",
              " 'health_statistics/obesity_age/2014.csv',\n",
              " 'health_statistics/obesity_age/2016.csv',\n",
              " 'health_statistics/obesity_age/2017.csv',\n",
              " 'health_statistics/obesity_age/2013.csv',\n",
              " 'health_statistics/obesity_age/2007.csv',\n",
              " 'health_statistics/obesity_age/2006.csv',\n",
              " 'health_statistics/obesity_age/2012.csv',\n",
              " 'health_statistics/obesity_age/2004.csv',\n",
              " 'health_statistics/obesity_age/2010.csv',\n",
              " 'health_statistics/obesity_age/2011.csv',\n",
              " 'health_statistics/obesity_age/2005.csv',\n",
              " 'health_statistics/exercise_gender/2008.csv',\n",
              " 'health_statistics/exercise_gender/2009.csv',\n",
              " 'health_statistics/exercise_gender/2018.csv',\n",
              " 'health_statistics/exercise_gender/2015.csv',\n",
              " 'health_statistics/exercise_gender/2014.csv',\n",
              " 'health_statistics/exercise_gender/2016.csv',\n",
              " 'health_statistics/exercise_gender/2017.csv',\n",
              " 'health_statistics/exercise_gender/2013.csv',\n",
              " 'health_statistics/exercise_gender/2007.csv',\n",
              " 'health_statistics/exercise_gender/2006.csv',\n",
              " 'health_statistics/exercise_gender/2012.csv',\n",
              " 'health_statistics/exercise_gender/2004.csv',\n",
              " 'health_statistics/exercise_gender/2010.csv',\n",
              " 'health_statistics/exercise_gender/2011.csv',\n",
              " 'health_statistics/exercise_gender/2005.csv',\n",
              " 'health_statistics/exercise_overall/2008.csv',\n",
              " 'health_statistics/exercise_overall/2009.csv',\n",
              " 'health_statistics/exercise_overall/2018.csv',\n",
              " 'health_statistics/exercise_overall/2015.csv',\n",
              " 'health_statistics/exercise_overall/2014.csv',\n",
              " 'health_statistics/exercise_overall/2016.csv',\n",
              " 'health_statistics/exercise_overall/2017.csv',\n",
              " 'health_statistics/exercise_overall/2013.csv',\n",
              " 'health_statistics/exercise_overall/2007.csv',\n",
              " 'health_statistics/exercise_overall/2006.csv',\n",
              " 'health_statistics/exercise_overall/2012.csv',\n",
              " 'health_statistics/exercise_overall/2004.csv',\n",
              " 'health_statistics/exercise_overall/2010.csv',\n",
              " 'health_statistics/exercise_overall/2011.csv',\n",
              " 'health_statistics/exercise_overall/2005.csv',\n",
              " 'health_statistics/obesity_overall/2008.csv',\n",
              " 'health_statistics/obesity_overall/2009.csv',\n",
              " 'health_statistics/obesity_overall/2018.csv',\n",
              " 'health_statistics/obesity_overall/2015.csv',\n",
              " 'health_statistics/obesity_overall/2014.csv',\n",
              " 'health_statistics/obesity_overall/2016.csv',\n",
              " 'health_statistics/obesity_overall/2017.csv',\n",
              " 'health_statistics/obesity_overall/2013.csv',\n",
              " 'health_statistics/obesity_overall/2007.csv',\n",
              " 'health_statistics/obesity_overall/2006.csv',\n",
              " 'health_statistics/obesity_overall/2012.csv',\n",
              " 'health_statistics/obesity_overall/2004.csv',\n",
              " 'health_statistics/obesity_overall/2010.csv',\n",
              " 'health_statistics/obesity_overall/2011.csv',\n",
              " 'health_statistics/obesity_overall/2005.csv',\n",
              " 'health_statistics/exercise_age/2008.csv',\n",
              " 'health_statistics/exercise_age/2009.csv',\n",
              " 'health_statistics/exercise_age/2018.csv',\n",
              " 'health_statistics/exercise_age/2015.csv',\n",
              " 'health_statistics/exercise_age/2014.csv',\n",
              " 'health_statistics/exercise_age/2016.csv',\n",
              " 'health_statistics/exercise_age/2017.csv',\n",
              " 'health_statistics/exercise_age/2013.csv',\n",
              " 'health_statistics/exercise_age/2007.csv',\n",
              " 'health_statistics/exercise_age/2006.csv',\n",
              " 'health_statistics/exercise_age/2012.csv',\n",
              " 'health_statistics/exercise_age/2004.csv',\n",
              " 'health_statistics/exercise_age/2010.csv',\n",
              " 'health_statistics/exercise_age/2011.csv',\n",
              " 'health_statistics/exercise_age/2005.csv',\n",
              " 'health_statistics/obesity_gender/2008.csv',\n",
              " 'health_statistics/obesity_gender/2009.csv',\n",
              " 'health_statistics/obesity_gender/2018.csv',\n",
              " 'health_statistics/obesity_gender/2015.csv',\n",
              " 'health_statistics/obesity_gender/2014.csv',\n",
              " 'health_statistics/obesity_gender/2016.csv',\n",
              " 'health_statistics/obesity_gender/2017.csv',\n",
              " 'health_statistics/obesity_gender/2013.csv',\n",
              " 'health_statistics/obesity_gender/2007.csv',\n",
              " 'health_statistics/obesity_gender/2006.csv',\n",
              " 'health_statistics/obesity_gender/2012.csv',\n",
              " 'health_statistics/obesity_gender/2004.csv',\n",
              " 'health_statistics/obesity_gender/2010.csv',\n",
              " 'health_statistics/obesity_gender/2011.csv',\n",
              " 'health_statistics/obesity_gender/2005.csv']"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "5lHr1i-Eq2ID"
      },
      "outputs": [],
      "source": [
        "# Dont change the output directory and save your output files to this folder\n",
        "output_directory = \"preprocessed_data/\"\n",
        "\n",
        "def clean(spatial_paths, temporal_paths, stats_paths):\n",
        "  # Write you solution here\n",
        "  ############# SOLUTION ###############\n",
        "  # --- Temporal Data ---\n",
        "  temporal_df = pd.DataFrame()\n",
        "  \n",
        "  # Read first file and add it to the dataframe\n",
        "  temporal_df = pd.read_csv(input_directory + temporal_paths[0])\n",
        "  \n",
        "  # Pre-emtively create a date column by converting the 'date' column to datetime\n",
        "  # This prevents an error I was getting when parsing dates from all files and converting to datetime\n",
        "  temporal_df['date'] = pd.to_datetime(temporal_df['date'], format='%Y-%m-%d')\n",
        "  \n",
        "  # Drop the isPartial column since it is not required\n",
        "  temporal_df = temporal_df.drop(columns=['isPartial'])\n",
        "\n",
        "  # Go through all other files and merge them into our dataframe\n",
        "  for path in temporal_paths[1:]:\n",
        "    df = pd.read_csv(input_directory + path)\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
        "    df = df.drop(columns=['isPartial'], errors='ignore')\n",
        "    temporal_df = pd.merge(temporal_df, df, on='date', how='outer')\n",
        "\n",
        "  temporal_df.to_csv(output_directory + 'temporal_trends.csv', index=False)\n",
        "  \n",
        "  print(f'Shape of temporal_df: {temporal_df.shape}') # Expected: (180, 82)\n",
        "  # --- Temporal Data ---\n",
        "  \n",
        "  # --- Spatial Data ---\n",
        "  final_spatial_df = pd.DataFrame()  # This will hold all the years' data\n",
        "\n",
        "  years = list(range(2004, 2019))\n",
        "  for year in years:\n",
        "    # Filter files for the current year\n",
        "    current_year_files = [f for f in spatial_paths if str(year) in f]\n",
        "        \n",
        "    # Start with the first file for the year\n",
        "    base_file = current_year_files[0]\n",
        "    base_df = pd.read_csv(input_directory + base_file)\n",
        "    base_df['year'] = year  # Add the year column\n",
        "        \n",
        "    # Iterate through all other files for the year and merge them\n",
        "    for file in current_year_files[1:]:\n",
        "      current_df = pd.read_csv(input_directory + file)\n",
        "            \n",
        "      # Extract keyword from filename (removing the 'year_spatial_' prefix and '.csv' suffix)\n",
        "      keyword = file.split('/')[-1].replace(f\"{year}_spatial_\", \"\").replace(\".csv\", \"\")\n",
        "            \n",
        "      # Rename the second column to the keyword\n",
        "      current_df.columns = [\"geoName\", keyword]\n",
        "            \n",
        "      # Merge with base dataframe\n",
        "      base_df = pd.merge(base_df, current_df, on='geoName', how='left')\n",
        "            \n",
        "    # Append the data for this year to the final dataframe\n",
        "    final_spatial_df = pd.concat([final_spatial_df, base_df], ignore_index=True)\n",
        "\n",
        "  # Sort the dataframe\n",
        "  final_spatial_df = final_spatial_df.sort_values(by=['year', 'geoName'])\n",
        "  \n",
        "  # Reordering columns\n",
        "  column_order = [\"geoName\", \"year\"]\n",
        "  sorted_keywords = sorted([col for col in final_spatial_df.columns if col not in column_order])\n",
        "  column_order.extend(sorted_keywords)\n",
        "  final_spatial_df = final_spatial_df[column_order]\n",
        "\n",
        "  # Save the consolidated dataframe to the specified CSV file\n",
        "  final_spatial_df.to_csv(output_directory + 'spatial_trends.csv', index=False)\n",
        "  \n",
        "  print(f'Shape of spatial_df: {final_spatial_df.shape}') # Expected: (765, 83)\n",
        "  # --- Spatial Data ---\n",
        "  \n",
        "  # --- Health Statistics Data ---\n",
        "  # Go through all paths in the file and merge them into a single dataframe\n",
        "  health_stats_df = pd.DataFrame()\n",
        "  for path in stats_paths:\n",
        "    df = pd.read_csv(input_directory + path)\n",
        "    variable = path.split('/')[1].split('_')[0]\n",
        "    df['variable'] = variable\n",
        "    health_stats_df = pd.concat([health_stats_df, df], ignore_index=True)\n",
        "  \n",
        "  # Clean up column names in health_stats_df by making them lowercase and removing spaces\n",
        "  health_stats_df.columns = [col.lower().strip() for col in health_stats_df.columns]\n",
        "  \n",
        "  # List of columns to keep\n",
        "  columns_to_keep = [\n",
        "    \"id\",\n",
        "    \"year\",\n",
        "    \"locationabbr\",\n",
        "    \"locationdesc\",\n",
        "    \"data_value\",\n",
        "    \"low_confidence_limit\",\n",
        "    \"high_confidence_limit\",\n",
        "    \"sample_size\",\n",
        "    \"stratification\",\n",
        "    \"stratificationtype\",\n",
        "    \"variable\"\n",
        "  ]\n",
        "  \n",
        "  health_stats_df = health_stats_df[columns_to_keep]\n",
        "  \n",
        "  # Filter out uninformative rows based on 'id' column\n",
        "  health_stats_df = health_stats_df[health_stats_df['id'].notna()]\n",
        "  \n",
        "  # Sort the rows by “variable”, then “year”, and then “id” in the ascending order\n",
        "  health_stats_df = health_stats_df.sort_values(by=[\"variable\", \"year\", \"id\"])\n",
        "    \n",
        "  health_stats_df.to_csv(output_directory + 'health_stats.csv', index=False)\n",
        "\n",
        "  print(f'Shape of health_stats_df: {health_stats_df.shape}') # Expected: (14442, 11)\n",
        "  \n",
        "  # --- Health Statistics Data ---\n",
        "  \n",
        "  ############# SOLUTION END ###############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Rmdf_o68BXvm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of temporal_df: (180, 82)\n",
            "Shape of spatial_df: (765, 83)\n",
            "Shape of health_stats_df: (14442, 11)\n"
          ]
        }
      ],
      "source": [
        "# How we will call your function\n",
        "\n",
        "clean(spatial_paths, temporal_paths, stats_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGyqP7dnRhU"
      },
      "source": [
        "## *Concepts required to complete this task*\n",
        "\n",
        "*   Data Merging\n",
        "*   Data Concatenation\n",
        "*   Dataframes Indexing, Sorting, Method Chaining, and Manipulation\n",
        "*   Reading and Writing CSV files as Dataframes and vice-versa.\n",
        "*   (Optionally) `reduce()` function from `functools`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmqfUZGwyLB"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +8 points for correctness of the `temporal.csv` file (as per the specifications of the README file, and by using correct pandas techniques)\n",
        "- +8 points for correctness of the `spatial.csv` file (as per the specifications of the README file, and by using pandas)\n",
        "- +14 points for correctness of the `health.csv` file (as per the specifications of the README file, and by using pandas)\n",
        "- +5 points for proper comments and variable names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuRYqcxmn4vS"
      },
      "source": [
        "# Part II: MovieLens Dataset: Popular Movies and Biases (35 points)\n",
        "\n",
        "In this part, you will be manipulating the **MovieLens Dataset**. Before moving on to the prompts for this section, please read the **README.txt** file provided with the dataset. Also visually inspect the different files in the dataset along with the fields/columns in each file to get a sense of how the data looks like.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYvBKcPSngP"
      },
      "source": [
        "## A. Most Popular Movies (15 points)\n",
        "\n",
        "Naturally, with the movie ratings dataset, we would like to first know which movies were rated the highest, and what were their genres. To be able to do that, we would like to\n",
        "\n",
        "1. Create a DataFrame with the following six columns:\n",
        "  *   **movie_id**: this is the unique identifier of the movie as provided in the dataset\n",
        "  *   **movie_title**: this is the title of the movie as provided in the dataset\n",
        "  *   **release_date**: this is the date of release as provided in the dataset\n",
        "  *   **genre(s)**: this is the genre of the movie or combination of genres. This is not a 0 or 1 value, but the actual name of the genre.\n",
        "\n",
        "      *Note: If the movie has more than one genre, those genres should be appended with \"and\" in order to create new genres. For example, if the movie genres was Comedy, Drama and Action, you would need to combine this to create a new genre called \"Action and Comedy and Drama\". You should combine them in (alphabetical or some other fixed) order so that if you ever need to group them by genres, it's easier and meaningful to do so. In other words, you would not want both \"Action and Comedy and Drama\" as well as \"Comedy and Action and Drama\" as they are practically the same groups*\n",
        "  *   **average_rating**: this is the average rating of the movie computed by averaging all the ratings given\n",
        "  *   **number_of_raters**: this is the number of raters for each movie\n",
        "\n",
        "2. We would like to then filter out/remove movies with just `unknown` genre, movies with less than 50 raters, and movies with average rating of less than 3.\n",
        "\n",
        "3. Finally, we would like to sort the DataFrame in the descending order by `average_rating` to view the top rated movies.\n",
        "\n",
        "You solution should make use of Pandas functions wherever necessary, and use no loops at all.\n",
        "\n",
        "Your code should not be more than 15 lines/statements of code. Our reference solution is 5 lines of code, aside from the 4 lines of reading files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "MJuCSn4JSqWI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>movie_title</th>\n",
              "      <th>release_date</th>\n",
              "      <th>genre(s)</th>\n",
              "      <th>average_rating</th>\n",
              "      <th>number_of_raters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>408</td>\n",
              "      <td>Close Shave, A (1995)</td>\n",
              "      <td>28-Apr-1996</td>\n",
              "      <td>Animation and Comedy and Thriller</td>\n",
              "      <td>4.491071</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>318</td>\n",
              "      <td>Schindler's List (1993)</td>\n",
              "      <td>01-Jan-1993</td>\n",
              "      <td>Drama and War</td>\n",
              "      <td>4.466443</td>\n",
              "      <td>298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>169</td>\n",
              "      <td>Wrong Trousers, The (1993)</td>\n",
              "      <td>01-Jan-1993</td>\n",
              "      <td>Animation and Comedy</td>\n",
              "      <td>4.466102</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>483</td>\n",
              "      <td>Casablanca (1942)</td>\n",
              "      <td>01-Jan-1942</td>\n",
              "      <td>Drama and Romance and War</td>\n",
              "      <td>4.456790</td>\n",
              "      <td>243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>114</td>\n",
              "      <td>Wallace &amp; Gromit: The Best of Aardman Animatio...</td>\n",
              "      <td>05-Apr-1996</td>\n",
              "      <td>Animatio</td>\n",
              "      <td>4.447761</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>702</td>\n",
              "      <td>Barcelona (1994)</td>\n",
              "      <td>01-Jan-1994</td>\n",
              "      <td>Comedy and Romance</td>\n",
              "      <td>3.018868</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>476</td>\n",
              "      <td>First Wives Club, The (1996)</td>\n",
              "      <td>14-Sep-1996</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>3.018750</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1047</th>\n",
              "      <td>1048</td>\n",
              "      <td>She's the One (1996)</td>\n",
              "      <td>23-Aug-1996</td>\n",
              "      <td>Comedy and Romance</td>\n",
              "      <td>3.013699</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>Net, The (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>Sci-Fi and Thriller</td>\n",
              "      <td>3.008333</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>780</td>\n",
              "      <td>Dumb &amp; Dumber (1994)</td>\n",
              "      <td>01-Jan-1994</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>529 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      movie_id                                        movie_title  \\\n",
              "407        408                              Close Shave, A (1995)   \n",
              "317        318                            Schindler's List (1993)   \n",
              "168        169                         Wrong Trousers, The (1993)   \n",
              "482        483                                  Casablanca (1942)   \n",
              "113        114  Wallace & Gromit: The Best of Aardman Animatio...   \n",
              "...        ...                                                ...   \n",
              "701        702                                   Barcelona (1994)   \n",
              "475        476                       First Wives Club, The (1996)   \n",
              "1047      1048                               She's the One (1996)   \n",
              "37          38                                    Net, The (1995)   \n",
              "779        780                               Dumb & Dumber (1994)   \n",
              "\n",
              "     release_date                           genre(s)  average_rating  \\\n",
              "407   28-Apr-1996  Animation and Comedy and Thriller        4.491071   \n",
              "317   01-Jan-1993                      Drama and War        4.466443   \n",
              "168   01-Jan-1993               Animation and Comedy        4.466102   \n",
              "482   01-Jan-1942          Drama and Romance and War        4.456790   \n",
              "113   05-Apr-1996                           Animatio        4.447761   \n",
              "...           ...                                ...             ...   \n",
              "701   01-Jan-1994                 Comedy and Romance        3.018868   \n",
              "475   14-Sep-1996                             Comedy        3.018750   \n",
              "1047  23-Aug-1996                 Comedy and Romance        3.013699   \n",
              "37    01-Jan-1995                Sci-Fi and Thriller        3.008333   \n",
              "779   01-Jan-1994                             Comedy        3.000000   \n",
              "\n",
              "      number_of_raters  \n",
              "407                112  \n",
              "317                298  \n",
              "168                118  \n",
              "482                243  \n",
              "113                 67  \n",
              "...                ...  \n",
              "701                 53  \n",
              "475                160  \n",
              "1047                73  \n",
              "37                 120  \n",
              "779                 69  \n",
              "\n",
              "[529 rows x 6 columns]"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write you solution here\n",
        "\n",
        "############# SOLUTION ###############\n",
        "# Read the CSV files\n",
        "users = pd.read_csv('movielens/user.csv', sep='\\t', header=None, names=['user_id', 'age', 'gender', 'occupation', 'zip_code'], encoding=\"ISO-8859-1\")\n",
        "movies = pd.read_csv('movielens/movie.csv', sep='\\t', header=None, names=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL'] + list(pd.read_csv('movielens/genre.csv', sep='\\t', header=None)[0].values), encoding=\"ISO-8859-1\")\n",
        "data = pd.read_csv('movielens/data.csv', sep='\\t', header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'], encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Create a DataFrame with desired columns\n",
        "movie_ratings = data.groupby('movie_id').agg(average_rating=('rating', 'mean'), number_of_raters=('rating', 'count')).reset_index()\n",
        "merged = movies.merge(movie_ratings, on='movie_id', how='left')\n",
        "merged['genre(s)'] = merged.drop(columns=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'average_rating', 'number_of_raters']).dot(pd.read_csv('movielens/genre.csv', sep='\\t', header=None).set_index(0).index + ' and ').str.rstrip(' and ')\n",
        "\n",
        "# Filter and sort the dataframe\n",
        "result = merged[['movie_id', 'movie_title', 'release_date', 'genre(s)', 'average_rating', 'number_of_raters']]\n",
        "result = result[(result['genre(s)'] != 'unknown') & (result['number_of_raters'] >= 50) & (result['average_rating'] >= 3)].sort_values(by='average_rating', ascending=False)\n",
        "\n",
        "result\n",
        "\n",
        "\n",
        "############# SOLUTION END ############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfxHQW3wjyQ"
      },
      "source": [
        "### *Concepts required to complete this task*\n",
        "\n",
        "*   Reading CSV files using Pandas\n",
        "*   Method Chaining\n",
        "*   Merging and Concatenation\n",
        "*   (Boolean) Indexing\n",
        "*   Data Manipulation Using Pandas\n",
        "*   Replacing\n",
        "*   Applying\n",
        "*   Aggregating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIHTCLCwiO7"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +10 points for correctness using Pandas library\n",
        "- +3 points for conciseness\n",
        "- +2 points for proper comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSiXTuoSuV-"
      },
      "source": [
        "## B. Does gender affect rating? (20 points)\n",
        "\n",
        "People perceive things differently based on their identity, culture, age, gender, and values. In [some studies](https://en.wikipedia.org/wiki/Complimentary_language_and_gender), women are known to give and receive more compliments than men. In terms of culture, [Germans are stereotypically perceived as stiff and more critical](https://en.wikipedia.org/wiki/Stereotypes_of_Germans). Furthermore, there may be conceptual differences: a rating of \"3\" on a particular movie may mean just \"OK\" for one person, and may mean \"Good\" for another. Some people may just be highly optimistic/positive,and never assign a rating of less than \"4\". Some people may just hate certain genres, and like the others. Notice how in such cases ratings are not dependent on the movie (which is the main goal) but on the characteristics/traits of the users.\n",
        "\n",
        "In data, these issues are sometimes known as *annotator biases* or *rater biases*, and the characteristics of people that define these biases are known as *covariates*. In an ideal case, (movie) ratings should be independent of these biases. But that is rarely the case. Characterizing and understanding these differences is a challenging problem. However, we generally like to account for these differences and control for them: accounting/normalizing for them when creating a machine learning model *(as we will see later in the course)*, and controlling for them when studying relationships between variables *(as you may learn if you take a causal inference/computational social science course)*.\n",
        "\n",
        "Age and gender are classic covariates. That is the reason why you would notice that most datasets like *MovieLens* tend to provide information about their raters'/subjects' gender, age, location, and so on. Controlling for such *covariates/confounders* is beyond the scope of this course.\n",
        "\n",
        "That said, in this task, we would like to do two very simple experiments to just inspect if there are biases introduced by gender in the dataset:\n",
        "\n",
        "(i) For each **movie title**, we would like to get mean ratings per gender; and then compute the absolute difference in mean ratings. While this by itself is not enough, if by visual inspection most movies have higher difference in mean ratings, that could signal towards gender bias;\n",
        "\n",
        "(ii) Regardless of the movie, for each **genre** (except the `unknown` genre), we would also like to compute the mean ratings per gender, and then compute the absolute difference in mean ratings.\n",
        "\n",
        "But how would you group movies by genre when one movie can have multiple genres, and especially **different number** of genres. Well, that is exactly why we made you do Part A to sort of group different combinations of genres to create new ones :-).\n",
        "\n",
        "In the end, we want two DataFrames:\n",
        "\n",
        "(i) one named `df_movie_gender` with the following columns:\n",
        "\n",
        "  *   **movie_title**: this is the title of the movie\n",
        "  *   **difference in average ratings**: this is the absolute difference in average ratings of male and female raters for each movie\n",
        "\n",
        "(ii) second named `df_genre_gender` with the following columns:\n",
        "\n",
        "  *   **genre**: this is the name of the genre (or a composite genre)\n",
        "  *   **difference in average ratings**: this is the absolute difference in average ratings of male and female raters for each genre\n",
        "\n",
        "Both of your dataframes should be sorted by the column **difference in average ratings** in descending order.\n",
        "\n",
        "Note: Just by doing the above, you may not be able to visually tell if there is a significant difference in terms of ratings of different genders; for that you will have to do significance testing, which is not part of this assignment, but something you will learn soon. For now, we basically mean to compute the differences in the average ratings of male versus female for different movies, and for different genres to visually inspect if we consistently see large differences or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Note:\n",
        "\n",
        "Based on a discussion with Prof. Bedoor, I have used the `result` dataframe from part A to form the `df_movie_gender` and `df_genre_gender` dataframes. While we can use the csv files provided to form the required dataframes, I believe that the solution remains considerably more concise and readable if we use the `result` dataframe from part A, especially while creating the `df_genre_gender` dataframe where a given movie can belong to multiple genres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "dOomc_dhSwte"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>gender</th>\n",
              "      <th>movie_title</th>\n",
              "      <th>difference in average ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>Ran (1985)</td>\n",
              "      <td>1.107143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>Jane Eyre (1996)</td>\n",
              "      <td>1.067368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>Postman, The (1997)</td>\n",
              "      <td>1.066924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>First Knight (1995)</td>\n",
              "      <td>1.036577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Cook the Thief His Wife &amp; Her Lover, The (1989)</td>\n",
              "      <td>0.927363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Fish Called Wanda, A (1988)</td>\n",
              "      <td>0.001099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Circle of Friends (1995)</td>\n",
              "      <td>0.000693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>Firm, The (1993)</td>\n",
              "      <td>0.000483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>Return of the Jedi (1983)</td>\n",
              "      <td>0.000232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>His Girl Friday (1940)</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>527 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "gender                                      movie_title  \\\n",
              "379                                          Ran (1985)   \n",
              "257                                    Jane Eyre (1996)   \n",
              "363                                 Postman, The (1997)   \n",
              "177                                 First Knight (1995)   \n",
              "111     Cook the Thief His Wife & Her Lover, The (1989)   \n",
              "..                                                  ...   \n",
              "179                         Fish Called Wanda, A (1988)   \n",
              "95                             Circle of Friends (1995)   \n",
              "176                                    Firm, The (1993)   \n",
              "391                           Return of the Jedi (1983)   \n",
              "230                              His Girl Friday (1940)   \n",
              "\n",
              "gender  difference in average ratings  \n",
              "379                          1.107143  \n",
              "257                          1.067368  \n",
              "363                          1.066924  \n",
              "177                          1.036577  \n",
              "111                          0.927363  \n",
              "..                                ...  \n",
              "179                          0.001099  \n",
              "95                           0.000693  \n",
              "176                          0.000483  \n",
              "391                          0.000232  \n",
              "230                          0.000000  \n",
              "\n",
              "[527 rows x 2 columns]"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write you solution here\n",
        "df_movie_gender = None\n",
        "############# SOLUTION FOR df_movie_gender ###############\n",
        "# Merge results from the previous part with users dataframe\n",
        "merged = result.merge(data, on='movie_id', how='left').merge(users, on='user_id', how='left')\n",
        "merged = merged.drop(columns=['timestamp', 'zip_code', 'occupation', 'age', 'average_rating'])\n",
        "\n",
        "# Group movies by title and gender and calculate the average rating\n",
        "avg_ratings = merged.groupby(['movie_title', 'gender'])['rating'].mean().unstack()\n",
        "\n",
        "# Use the gender-wise average ratings to calculate the difference in average ratings\n",
        "avg_ratings['difference in average ratings'] = (avg_ratings['M'] - avg_ratings['F']).abs()\n",
        "df_movie_gender = avg_ratings.reset_index()[['movie_title', 'difference in average ratings']]\n",
        "\n",
        "# Sort the dataframe by the difference in average ratings in descending order\n",
        "df_movie_gender = df_movie_gender.sort_values(by='difference in average ratings', ascending=False)\n",
        "\n",
        "df_movie_gender\n",
        "############# SOLUTION END ############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "wgKws4gsoHqP"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>gender</th>\n",
              "      <th>genre(s)</th>\n",
              "      <th>difference in average ratings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Action and Adventure and Drama and Romance</td>\n",
              "      <td>1.036577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Action and Wester</td>\n",
              "      <td>0.762913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Action and Crime and Romance</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Sci-Fi and Thriller</td>\n",
              "      <td>0.625616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Drama and Mystery and Sci-Fi and Thriller</td>\n",
              "      <td>0.612732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Children's and Drama and Fantasy and Sci-Fi</td>\n",
              "      <td>0.008095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Drama and Fantasy and Thriller</td>\n",
              "      <td>0.006410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>Horror</td>\n",
              "      <td>0.002144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Action and Drama and Romance</td>\n",
              "      <td>0.001981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>Drama and Thriller</td>\n",
              "      <td>0.001832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "gender                                     genre(s)  \\\n",
              "12       Action and Adventure and Drama and Romance   \n",
              "52                                Action and Wester   \n",
              "28                     Action and Crime and Romance   \n",
              "146                             Sci-Fi and Thriller   \n",
              "117       Drama and Mystery and Sci-Fi and Thriller   \n",
              "..                                              ...   \n",
              "80      Children's and Drama and Fantasy and Sci-Fi   \n",
              "113                  Drama and Fantasy and Thriller   \n",
              "134                                          Horror   \n",
              "33                     Action and Drama and Romance   \n",
              "125                              Drama and Thriller   \n",
              "\n",
              "gender  difference in average ratings  \n",
              "12                           1.036577  \n",
              "52                           0.762913  \n",
              "28                           0.727273  \n",
              "146                          0.625616  \n",
              "117                          0.612732  \n",
              "..                                ...  \n",
              "80                           0.008095  \n",
              "113                          0.006410  \n",
              "134                          0.002144  \n",
              "33                           0.001981  \n",
              "125                          0.001832  \n",
              "\n",
              "[150 rows x 2 columns]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write you solution here\n",
        "df_genre_gender = None\n",
        "############# SOLUTION FOR df_genre_gender ###############\n",
        "# Splitting the composite genres into separate rows\n",
        "df_expanded_genres = merged.explode('genre(s)', ignore_index=True)\n",
        "\n",
        "# Group movies by gender and genre and calculate the average rating\n",
        "avg_ratings_genre = df_expanded_genres.groupby(['genre(s)', 'gender'])['rating'].mean().unstack()\n",
        "\n",
        "# Find the absolute difference in average ratings\n",
        "avg_ratings_genre['difference in average ratings'] = (avg_ratings_genre['M'] - avg_ratings_genre['F']).abs()\n",
        "df_genre_gender = avg_ratings_genre.reset_index()[['genre(s)', 'difference in average ratings']]\n",
        "\n",
        "# Sort the dataframe by the difference in average ratings in descending order\n",
        "df_genre_gender = df_genre_gender.sort_values(by='difference in average ratings', ascending=False)\n",
        "\n",
        "df_genre_gender\n",
        "\n",
        "############# SOLUTION End ###############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N08hHBwwoHHT"
      },
      "source": [
        "## *Concepts required to complete this task*\n",
        "\n",
        "*   Reading CSV files using Pandas\n",
        "*   Method Chaining\n",
        "*   Merging and Concatenation\n",
        "*   (Boolean) Indexing\n",
        "*   Data Manipulation Using Pandas\n",
        "*   Replacing\n",
        "*   Applying\n",
        "*   Aggregating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmxPH91wcq1"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +8 points for correctness of `df_movie_gender` using proper logic\n",
        "- +8 points for correctness of `df_genre_gender` using proper logic\n",
        "- +3 points for conciseness\n",
        "- +1 points for proper comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AORs-0rkn9Rf"
      },
      "source": [
        "# Part III: COVID-19 Dataset: Planning Average Joe's Vacation (30 points)\n",
        "\n",
        "Your rich and adventurous uncle named **Average Joe** does not believe in Coronavirus. He thinks COVID-19 is a hoax. He's fed-up of just sitting at home, and would love to travel. He wants to travel to a country that has consistently been less stringent in terms of its rules and policies. He has been nagging you to help him find such a country for 40 *not-so-magical* points in this assignment. You have no choice, but to help him. :-(\n",
        "\n",
        "In this part you will do that by manipulating the **Coronavirus Pandemic (COVID-19) Dataset**. Before moving on to the prompts in this section, please read the **README** file provided with the dataset. Also visually inspect the different files in the dataset along with the fields/columns in each file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWbNwL6SUFIj"
      },
      "source": [
        "## A. Least Stringent Nations (15 points)\n",
        "\n",
        "In this part, you will use the **Government Response Stringency Index** to figure out the least stringent nations. You would first compute the average stringency index for each country by month, and then you would `quantize` that.\n",
        "\n",
        "*Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set*.\n",
        "\n",
        "In this task, you will quantize these average stringency indices into three groups (less_stringent, somewhat_stringent, extremely_stringent) based on the following rules:\n",
        "\n",
        "  - less_stringent: average stringency index <= 40\n",
        "  - somewhat_stringent: average stringency index > 40 but <= 70\n",
        "  - extremely_stringent: average stringency index > 70\n",
        "\n",
        "Once you have grouped, aggregated, and quantized these values, we would like you to filter/remove all countries which have either *ever* been *extremely_stringent* or *ever* been *somewhat_stringent*, and provide a Series/list of the remaining countries. In other words, we want countries that have always been *less_stringent*.\n",
        "\n",
        "These are the countries, your uncle Joe will use to decide from.\n",
        "\n",
        "Your solution should not be more than 20 lines of code, and should use no loops at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick note:\n",
        "\n",
        "Since some countries in the dataset do not have complete or any data in the `stringency_index` column, I have removed the rows where the `stringency_index` is missing via the `notna()` function. This is done to ensure that the average stringency index is not affected by the missing values.\n",
        "\n",
        "However, this leads us to a scenario where some countries do not have data for stringency for some months of the year. For the purposes of this assignment, I will consider countries that are less stringent for the months where they have data, even if they do not have data for all months of the year (for example: Vanuatu has stringency data for just 3 months of the year, and they have been less stringent for those months, so I will consider Vanuatu as a less stringent country)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "AcqdJNb-vtIt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Less stringent countries:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso_code</th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>BDI</td>\n",
              "      <td>Burundi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>BLR</td>\n",
              "      <td>Belarus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>FRO</td>\n",
              "      <td>Faeroe Islands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>GRL</td>\n",
              "      <td>Greenland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>NIC</td>\n",
              "      <td>Nicaragua</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1895</th>\n",
              "      <td>TWN</td>\n",
              "      <td>Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>VUT</td>\n",
              "      <td>Vanuatu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     iso_code        location\n",
              "108       BDI         Burundi\n",
              "215       BLR         Belarus\n",
              "672       FRO  Faeroe Islands\n",
              "758       GRL       Greenland\n",
              "1356      NIC       Nicaragua\n",
              "1895      TWN          Taiwan\n",
              "2003      VUT         Vanuatu"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write you solution here\n",
        "\n",
        "############# SOLUTION ###############\n",
        "# Load the dataset\n",
        "owid_data = pd.read_csv(\"coronavirus_pandemic/owid-covid-data.csv\")\n",
        "\n",
        "# Remove rows where stringency index is null\n",
        "owid_data = owid_data[owid_data['stringency_index'].notna()]\n",
        "\n",
        "# Convert the date to a datetime object and extract the month\n",
        "owid_data['date'] = pd.to_datetime(owid_data['date'])\n",
        "owid_data['month'] = owid_data['date'].dt.month\n",
        "\n",
        "# Compute the average stringency index by country and month\n",
        "avg_stringency = owid_data.groupby(['iso_code', 'location', 'month'])['stringency_index'].mean().reset_index()\n",
        "\n",
        "# Quantize the stringency index\n",
        "conditions = [\n",
        "    avg_stringency['stringency_index'] <= 40,\n",
        "    (avg_stringency['stringency_index'] > 40) & (avg_stringency['stringency_index'] <= 70),\n",
        "    avg_stringency['stringency_index'] > 70\n",
        "]\n",
        "\n",
        "values = ['less_stringent', 'somewhat_stringent', 'extremely_stringent']\n",
        "\n",
        "avg_stringency['stringency_category'] = pd.cut(avg_stringency['stringency_index'], \n",
        "                                               bins=[0, 40, 70, 100], \n",
        "                                               labels=values, \n",
        "                                               right=False)\n",
        "\n",
        "# Filter out countries that have ever been 'extremely_stringent' or 'somewhat_stringent'\n",
        "non_stringent_countries = avg_stringency[~avg_stringency['iso_code'].isin(\n",
        "    avg_stringency[avg_stringency['stringency_category'].isin(['extremely_stringent', 'somewhat_stringent'])]['iso_code'])]\n",
        "\n",
        "non_stringent_countries = non_stringent_countries[['iso_code', 'location']].drop_duplicates()\n",
        "\n",
        "print(\"Less stringent countries:\")\n",
        "non_stringent_countries\n",
        "\n",
        "############# SOLUTION END ############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7-LyDQFwz5x"
      },
      "source": [
        "### *Concepts required to complete this task*\n",
        "\n",
        "*   Data Manipulation using Pandas\n",
        "*   Method Chaining\n",
        "*   Grouping\n",
        "*   Applying\n",
        "*   Aggregating\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtKyL2AUw2g3"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +10 points for correctness using Pandas library\n",
        "- +3 points for conciseness\n",
        "- +2 points for proper comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkKIEwMZUIaf"
      },
      "source": [
        "##B. Average Joe loves gatherings (10 points)\n",
        "\n",
        "**Average Joe** would also like to attend as many gatherings as possible wherever he travels, and would like to know the names of countries which have had **no restrictions on gatherings** for **most number of days** in the data.\n",
        "\n",
        "As part of this problem, your solution should result in a DataFrame that consists of the following columns:\n",
        "*   **country_name**: this is the name of the country\n",
        "*   **days_with_no_gathering_restrictions**: this is the number of days that the given country has had no restrictions on gatherings.\n",
        "\n",
        "The DataFrame should be sorted in descending order by the **days_with_no_gathering_restrictions** column.\n",
        "\n",
        "Your solution should be no more than 3 statements (this includes reading the file, manipulation, and sorting).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "C5NHj5RMUK-Q"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_name</th>\n",
              "      <th>days_with_no_gathering_restrictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Solomon Islands</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>Taiwan</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>Yemen</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Macao</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Kiribati</td>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Switzerland</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Iraq</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>China</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>Singapore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Comoros</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>185 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        country_name  days_with_no_gathering_restrictions\n",
              "146  Solomon Islands                                  398\n",
              "170           Taiwan                                  398\n",
              "181            Yemen                                  398\n",
              "103            Macao                                  398\n",
              "91          Kiribati                                  398\n",
              "..               ...                                  ...\n",
              "30       Switzerland                                   58\n",
              "80              Iraq                                   55\n",
              "32             China                                   21\n",
              "145        Singapore                                    0\n",
              "38           Comoros                                    0\n",
              "\n",
              "[185 rows x 2 columns]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write you solution here\n",
        "\n",
        "############# SOLUTION ###############\n",
        "# 1. Read the CSV file\n",
        "gathering_restrictions_df = pd.read_csv('coronavirus_pandemic/c4.csv')\n",
        "\n",
        "# 2. Compute the days with no gathering restrictions for each country\n",
        "gathering_restrictions_df['days_with_no_gathering_restrictions'] = (gathering_restrictions_df.iloc[:, 2:] == 0).sum(axis=1)\n",
        "\n",
        "# 3. Extract required columns and sort the dataframe\n",
        "sorted_gathering_restrictions_df = gathering_restrictions_df[['country_name', 'days_with_no_gathering_restrictions']].sort_values(by='days_with_no_gathering_restrictions', ascending=False)\n",
        "sorted_gathering_restrictions_df\n",
        "\n",
        "\n",
        "############# SOLUTION END ############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojM_ZaJUoWXw"
      },
      "source": [
        "### *Concepts required to complete this task*\n",
        "\n",
        "*   Data Manipulation Using Pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7k7Swj0w3K4"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +5 points for correctness using Pandas library\n",
        "- +3 points for conciseness\n",
        "- +2 points for proper comments and variable names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lCLP6iG-b_L"
      },
      "source": [
        "## C. Which country should he visit? (5 points)\n",
        "\n",
        "Based on the countries you yielded from Part A, and the ranking you computed from Part B, which country should be on the top list of priorities for Uncle Joe. This would be the country that will be the first country from the top in part B that is also present in the list of countries from Part A.\n",
        "\n",
        "\n",
        "Write 1 line of code that takes the list from Part A, DataFrame from Part B, and retrieves the top choice for the `country_name` along with its value for the `days_with_no_gathering_restrictions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "UpkpsHYXChr5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country: Taiwan | Days with no gathering restrictions: 398\n"
          ]
        }
      ],
      "source": [
        "# Write you solution here\n",
        "\n",
        "############# SOLUTION ###############\n",
        "# Find the first country in the sorted gathering restrictions dataframe that is in the list of non-stringent countries\n",
        "top_choice = sorted_gathering_restrictions_df[sorted_gathering_restrictions_df['country_name'].isin(non_stringent_countries['location'])].iloc[0]\n",
        "\n",
        "print(f'Country: {top_choice[\"country_name\"]} | Days with no gathering restrictions: {top_choice[\"days_with_no_gathering_restrictions\"]}')\n",
        "\n",
        "############# SOLUTION END ############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoLbz2y1EH0L"
      },
      "source": [
        "### *Concepts required to complete this task*\n",
        "\n",
        "*   Data Manipulation Using Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdrn4fV0D_p_"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +3 points for correctness\n",
        "- +2 points for conciseness"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUxdctIdxU6B"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
