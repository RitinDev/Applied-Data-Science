{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUxdctIdxU6B"
      },
      "source": [
        "---\n",
        "---\n",
        "Recitation 11: Machine Learning III\n",
        "\n",
        "Applied Data Science in Python for Social Scientists\n",
        "\n",
        "New York University, Abu Dhabi\n",
        "\n",
        "Dated: 23 Nov 2023\n",
        "---\n",
        "---\n",
        "#Start Here\n",
        "## Learning Goals\n",
        "### General Goals\n",
        "- Learn the fundamental concepts of applied machine learning\n",
        "- Learn the advanced machine learning supervised models\n",
        "\n",
        "### Specific Goals\n",
        "- Learn the basics of clustering\n",
        "- Learn to apply different models:\n",
        "    - Logistic Regression\n",
        "    - Support Vector Machines (SVMs)\n",
        "\n",
        "## Distribution of Class Materials\n",
        "These problem sets and recitations are intellectual property of NYUAD, and we request the students to **not** distribute them or their solutions to other students who have not signed up for this class, and/or intend to sign up in the future. We also request you don't post these problem sets, and recitations online or on any public platforms.\n",
        "\n",
        "## Submission\n",
        "You will submit all your code as a Python Notebook through BrightSpace as **R11_YOUR NETID.ipynb**.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMDlF4mjG6v"
      },
      "source": [
        "# General Instructions\n",
        "This recitation is worth 50 points. It has 2 parts. All the parts need to be completed in a Jupyter (Colab) Notebook attached with this handout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZsjCWi3lGi"
      },
      "source": [
        "# The World Before Word Embeddings\n",
        "\n",
        "\n",
        "If you have to create a classifier for **sentiment analysis**, and you have a movie reviews dataset *(aka a corpus)* with each review rated as **positive** or **negative**, which classifier would you use? Well, you could use a *multi-class logistic regression*, a *decision tree classifier* or an *SVM classifier*.\n",
        "\n",
        "Sure, but a problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer **well defined fixed-length features and outputs**. As you already know now, machine learning algorithms cannot work with raw text directly; **the text must be converted into numbers**. Specifically, **vectors of numbers**. So, **what will be your features?**\n",
        "\n",
        "Well remembering from PS7, you know that you can split each text review to a bunch of words, create **word embeddings/vectors** for each word, and concatenate those vectors to create a long vector for each review, and *ta-da!* that long vector will be your feature vector. Most recent classification algorithms in fact do something very similar.\n",
        "\n",
        "But the concept of word embeddings is relatively very new, and sentiment analysis or text processing is much older than that. So aside from word embeddings, **how can you extract features from text?**\n",
        "\n",
        "Well, pause for a second and think: what is the difference between a \"positive\", and a \"negative\" review?\n",
        "\n",
        "A positive review is likely to have more positive words such as *great*, *good*, *amazing*, and so on, whereas a negative reviews is likely to have more negative words such \"sucks\", \"awful\", and so on. This list of words that includes `[\"great\", \"good\", \"amazing\", \"sucks\", \"awful\"]` is your **vocabulary** of words that you are going to track. But are these words enough? How can you create a list of exhaustive words that generalizes across reviews? Think.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6qe9s-GwiBR"
      },
      "source": [
        "## Bag of Words (BoW)\n",
        "\n",
        "Well here's the solution. Ultimately, your machine learning model will learn from a training set right? And your training set includes a list of reviews. Why not create a list of all the unique words in your training set of reviews, and use that as your *vocabulary*?\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "Say your training set only has 3 reviews as follows:\n",
        "\n",
        "-----\n",
        "- **Review 1:** this movie is very scary and long\n",
        "- **Review 2:** this movie is not scary and is slow\n",
        "- **Review 3:** this movie is spooky and good\n",
        "-----\n",
        "\n",
        "In that case your vocabulary will consist of the following **11 words**:  \n",
        "\n",
        "`[\"this\", \"movie\", \"isâ€™\", \"very\", \"scary\", \"and\", \"long\", \"not\",  \"slow\", \"spooky\",  \"good\"]`.\n",
        "\n",
        "Finally, you can create **feature representation** for each review as follows:\n",
        "\n",
        "|   | 1 | 2  | 3  | 4  | 5  | 6  | 7  | 8  | 9  | 10  | 11  |   |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "|   | This  | movie  | is  | very  | scary  | and  | long  | not  | slow  |  spooky | good  | **Length of review**  |\n",
        "| Review 1  | 1  | 1  | 1  | 1  | 1 | 1  | 1  | 0  | 0  |  0 | 0  |  7 words  |\n",
        "| Review 2  | 1  | 1  | 2  | 0  | 1 | 1  | 0  | 1  | 1  | 0  | 0  | 8 words  |\n",
        "| Review 3  | 1  | 1  | 1  | 0  | 0 | 1  | 0  | 0  | 0  | 1  | 1  | 6 words  |\n",
        "\n",
        "\n",
        "You can notice that while review 1 has a length of 7 words, review 2 a length of 8 words, and review 3 a length of 6 words, the features for each review have a length of 11, and each feature is the count of the number of times the corresponding word occurs in the review. So if you notice in the table above, for Review 2, the word **\"is\"** occurs twice, and so the feature value is `2`.\n",
        "\n",
        "So the feature vectors based on the table for each review would be:\n",
        "\n",
        "**Feature Vector of Review 1:** `[1 1 1 1 1 1 1 0 0 0 0]`\n",
        "\n",
        "**Feature Vector of Review 2:** `[1 1 2 0 1 1 0 1 1 0 0]`\n",
        "\n",
        "**Feature Vector of Review 3:** `[1 1 1 0 0 1 0 0 0 1 1]`\n",
        "\n",
        "This approach is known as the **Bag of Words (BoW)** approach because what you are essentially doing is treating each review as a bag of words without accounting for the grammar or the order of words.\n",
        "\n",
        "What that means is that if, for example, Review 2 (i.e. *this movie is not scary and is slow*) was written as ***this is movie scary and is not slow*** which is grammatically incorrect and of different order in comparison from the original review, it won't matter as the feature vector will still remain the same as the words are still the same no matter the order.\n",
        "\n",
        "The figure below gives an overview of the BoW approach for a single review.\n",
        "\n",
        "![bow](https://drive.google.com/uc?id=1YF7mcm5hCTJc1WKb5Rvz2pxvUWDHUVdP)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AORs-0rkn9Rf"
      },
      "source": [
        "# Part I: Sentiment Analysis with BoW (35 points)\n",
        "\n",
        "For this recitation, you are given two csv files `sentiment_train.csv` and `sentiment_test.csv` for training and testing purposes. The csv file contains two columns: `Content` which includes the review in text, and `Label` which contains the sentiment label for the review: `pos` or `neg`. Your task is to extract features from the reviews given the Bag of Words approach, and use those features to create a sentiment classifier which, given a new text, outputs `pos` or `neg` based on if the review is positive or negative.\n",
        "\n",
        "Luckily, `scikit-learn` provides a method for extracting features from text. An example of how it works is given below. Read through the code with comments to understand what each line is doing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nx4ClsYc-UJx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importing the CountVectorizer method\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Creating the CountVectorizer object\n",
        "\"\"\"\n",
        "Notice the stop words argument. What this argument does is it ignores\n",
        "english stop words as they contribute very little. An example of a stop word\n",
        "would be words such as \"the\" or \"is\" or \"that\" -- these are words that you\n",
        "would see in any text and so won't help you in creating a classifier.\n",
        "By giving this flag you are sort of doing a feature selection by removing\n",
        "unnecessary features. You can play with this flag, and you will see that\n",
        "by removing this flag, your performance will go down.\n",
        "\"\"\"\n",
        "\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# An example list of text (otherwise called the corpus)\n",
        "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
        "        \"Look at it at least twice and definitely watch part 2.\",\n",
        "        \"It will change your view of the matrix.\",\n",
        "        \"Are the human people the ones who started the war?\",\n",
        "        \"Is AI a bad thing ?\"]\n",
        "\n",
        "\"\"\"\n",
        "Getting counts of each token (word) in text data (this returns a matrix).\n",
        "\n",
        "Notice that this is a `transform` -- very similar to how we were transforming\n",
        "when normalizing or standardizing, and so we would transform the test set\n",
        "similarly (as we were normalizing)\n",
        "based on the vocabulary (or distribution) of the training set. You can think\n",
        "for a second why that makes sense. (If we don't transform test based on train\n",
        "vocabulary then what if test has new vocabulary not present in train?)\n",
        "\"\"\"\n",
        "X = vect.fit_transform(corpus)\n",
        "\n",
        "\"\"\"\n",
        "Finally, converting sparse matrix to numpy array to view.\n",
        "You should note that the shape of this array is 5 x 35 as there are a total of\n",
        "35 words in our corpus, and 5 pieces of text in the list.\n",
        "\"\"\"\n",
        "\n",
        "X.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzt4Pn9nsuZb"
      },
      "source": [
        "## A. Using Logistic Regression (25 points)\n",
        "\n",
        "Using **Logistic Regression**, create a sentiment classifier that uses Bag of Words approach. Train your classifier on `sentiment_train.csv`, and test it on `sentiment_test.csv`. Use `accuracy_score` from `sklearn.metrics` to evaluate your classifier on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-BOTkY3iN5Zo"
      },
      "outputs": [],
      "source": [
        "# Importing libraries you would need\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H0qO6IPlQE3P"
      },
      "outputs": [],
      "source": [
        "input_directory = \"\"\n",
        "\n",
        "\n",
        "train = pd.read_csv(input_directory+\"sentiment_train.csv\")\n",
        "\n",
        "test = pd.read_csv(input_directory+\"sentiment_test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cuFhns1Xszu9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 100}\n",
            "LogisticRegression(C=100)\n"
          ]
        }
      ],
      "source": [
        "# Hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Feature extraction\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "X_train = vect.fit_transform(train['Content'])\n",
        "y_train = train['Label']\n",
        "\n",
        "# Model training with grid search\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "print(grid.best_estimator_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Kge33NbaTX7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.855\n"
          ]
        }
      ],
      "source": [
        "# Model evaluation\n",
        "X_test = vect.transform(test['Content'])\n",
        "y_test = test['Label']\n",
        "y_pred = grid.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Aka2n4dZT8"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +10 points for correctly extracting features from text for train and test set\n",
        "- +10 points for fitting the logistic regression model using grid search and a reasonable set of parameters\n",
        "- +5 points for evaluating the model on test set that performs reasonably well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dych6rCcspit"
      },
      "source": [
        "## B. Using SVM (10 points)\n",
        "\n",
        "Now implement the sentiment classifier using a Support Vector Machine (SVM) based classifier using linear kernel. For the sake of time, you don't have to use grid search here. Just use a `C` value of `0.001`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LZrWB7-KnPIc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.86\n"
          ]
        }
      ],
      "source": [
        "# Set the hyperparameter\n",
        "C = 0.001\n",
        "\n",
        "# Train the SVM with linear kernel\n",
        "SVM = svm.SVC(kernel='linear', C=C).fit(X_train, y_train)\n",
        "\n",
        "# Predict using the trained SVM model\n",
        "y_pred = SVM.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq3wrDzhdEUf"
      },
      "source": [
        "### Rubric\n",
        "\n",
        "- +5 points for writing code for training an SVM with linear kernel\n",
        "- +5 points for evaluating the model on test set that performs reasonably well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji1k5Zycs9xU"
      },
      "source": [
        "# Part II: Problems with Bag of Words (15 points)\n",
        "\n",
        "In less than 10 sentences, describe the problems with representing the text as a **Bag of Words** feature representation? Give examples where BoW approach may not work well. How can some of these issues be fixed? You may find [this](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) and [this](https://medium.com/voice-tech-podcast/nlp-pipeline-101-with-basic-code-example-feature-extraction-ea9894ed8daf) article useful in answering this question.\n",
        "\n",
        "The Bag of Words (BoW) model has several limitations:\n",
        "\n",
        "1. **Loss of Context**: It ignores the order of words, leading to a loss of contextual meaning. For example, \"not good\" and \"good\" may be treated similarly.\n",
        "2. **Synonymy and Polysemy Issues**: BoW cannot handle synonyms and polysemy, affecting the understanding of meanings.\n",
        "3. **High Dimensionality and Sparsity**: Results in large, sparse matrices which are computationally inefficient.\n",
        "4. **Neglects Negations and Modifiers**: Fails to account for words like \"not\" or \"very\" which alter meanings significantly.\n",
        "5. **Poor Handling of Rare Words**: Rare, yet contextually important words are often overlooked.\n",
        "6. **No Word Sense Disambiguation**: Cannot discern different meanings of a word in different contexts.\n",
        "\n",
        "To mitigate these issues, one can use:\n",
        "1. **Word Embeddings**: Techniques like Word2Vec or GloVe provide vector representations of words that capture semantic relationships and contexts.\n",
        "2. **Syntax Trees or Dependency Parsing**: These can be used to understand the grammatical structure of sentences, helping in understanding context and relationships between words.\n",
        "\n",
        "From my knowledge, n-grams and TF-IDF can also be used to improve the BoW model. However, my knowledge of these techniques is limited, so I will not discuss them in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpiIPvs4Lej7"
      },
      "source": [
        "## Rubric\n",
        "\n",
        "- +10 points for highlighting the problems with BoW approach with reasonable examples\n",
        "- +5 points for presenting alternative approach(es) to solving some of those issues"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUxdctIdxU6B"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
